{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are twelve different authors in the gutenberg corpus. These will be used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "poems = gutenberg.raw('blake-poems.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "parents = gutenberg.raw('edgeworth-parents.txt')\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "paradise = gutenberg.raw('milton-paradise.txt')\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "leaves = gutenberg.raw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "emma = re.sub(r'Chapter \\d+', '', emma)\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "sense = re.sub(r'Chapter \\d+', '', sense)\n",
    "bible = re.sub(r'Chapter \\d+', '', bible)\n",
    "poems = re.sub(r'Chapter \\d+', '', poems)\n",
    "stories = re.sub(r'Chapter \\d+', '', stories)\n",
    "busterbrown = re.sub(r'Chapter \\d+', '', busterbrown)\n",
    "alice = re.sub(r'Chapter \\d+', '', alice)\n",
    "ball = re.sub(r'Chapter \\d+', '', ball)\n",
    "brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "thursday = re.sub(r'Chapter \\d+', '', thursday)\n",
    "parents = re.sub(r'Chapter \\d+', '', parents)\n",
    "moby_dick = re.sub(r'Chapter \\d+', '', moby_dick)\n",
    "paradise = re.sub(r'Chapter \\d+', '', paradise)\n",
    "caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
    "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
    "leaves = re.sub(r'Chapter \\d+', '', leaves)\n",
    "\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)\n",
    "bible = text_cleaner(bible)\n",
    "poems = text_cleaner(poems)\n",
    "stories = text_cleaner(stories)\n",
    "busterbrown = text_cleaner(busterbrown)\n",
    "alice = text_cleaner(alice)\n",
    "ball = text_cleaner(ball)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)\n",
    "parents = text_cleaner(parents)\n",
    "moby_dick = text_cleaner(moby_dick)\n",
    "paradise = text_cleaner(paradise)\n",
    "caesar = text_cleaner(caesar)\n",
    "hamlet = text_cleaner(hamlet)\n",
    "macbeth = text_cleaner(macbeth)\n",
    "leaves = text_cleaner(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking to make sure the texts look correct:\n",
    "#print(emma)\n",
    "#print(persuasion)\n",
    "#print(sense)\n",
    "#print(bible[:1000])\n",
    "#print(poems)\n",
    "#print(stories)\n",
    "#print(busterbrown)\n",
    "#print(alice)\n",
    "#print(ball)\n",
    "#print(brown)\n",
    "#print(thursday)\n",
    "#print(parents)\n",
    "#print(moby_dick)\n",
    "#print(paradise)\n",
    "#print(caesar)\n",
    "#print(hamlet)\n",
    "#print(macbeth)\n",
    "#print(leaves)\n",
    "# The cleaner was successful for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)\n",
    "bible_doc = nlp(bible)\n",
    "poems_doc = nlp(poems)\n",
    "stories_doc = nlp(stories)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "alice_doc = nlp(alice)\n",
    "ball_doc = nlp(ball)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)\n",
    "parents_doc = nlp(parents)\n",
    "moby_dick_doc = nlp(moby_dick)\n",
    "paradise_doc = nlp(paradise)\n",
    "caesar_doc = nlp(caesar)\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)\n",
    "leaves_doc = nlp(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4  (Between, _, them, _, it, was, more, the, inti...  Austen"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "bible_sents = [[sent, \"KJV\"] for sent in bible_doc.sents]\n",
    "poems_sents = [[sent, \"Blake\"] for sent in poems_doc.sents]\n",
    "stories_sents = [[sent, \"Bryant\"] for sent in stories_doc.sents]\n",
    "busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "ball_sents = [[sent, \"Chesterton\"] for sent in ball_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "moby_dick_sents = [[sent, \"Melville\"] for sent in moby_dick_doc.sents]\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "caesar_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "hamlet_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "macbeth_sents = [[sent, \"Shakespeare\"] for sent in macbeth_doc.sents]\n",
    "leaves_sents = [[sent, \"Whitman\"] for sent in leaves_doc.sents]\n",
    "\n",
    "# Combine the sentences into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + sense_sents + bible_sents\n",
    "                         + poems_sents + stories_sents + busterbrown_sents\n",
    "                         + alice_sents + ball_sents + brown_sents + thursday_sents\n",
    "                         + parents_sents + moby_dick_sents + paradise_sents + \n",
    "                         caesar_sents + hamlet_sents + macbeth_sents + leaves_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78487</th>\n",
       "      <td>(But, ,, alas, !, the, practices, of, whalemen...</td>\n",
       "      <td>Melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95071</th>\n",
       "      <td>(Why, wag, your, head, with, turban, bound, ,,...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35652</th>\n",
       "      <td>(119:142, Thy, righteousness, is, an, everlast...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8607</th>\n",
       "      <td>(\", It, would, not, be, a, great, match, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>(And, God, said, unto, Jacob, ,, Arise, ,, go,...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0         1\n",
       "78487  (But, ,, alas, !, the, practices, of, whalemen...  Melville\n",
       "95071  (Why, wag, your, head, with, turban, bound, ,,...   Whitman\n",
       "35652  (119:142, Thy, righteousness, is, an, everlast...       KJV\n",
       "8607   (\", It, would, not, be, a, great, match, for, ...    Austen\n",
       "17175  (And, God, said, unto, Jacob, ,, Arise, ,, go,...       KJV"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using the full dataframe causes a memory error. \n",
    "### The below code will take a 30% sample of the dataframe.\n",
    "sample = sentences.sample(frac = .30, random_state = 25)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########## NEED TO DO BOW BEFORE CLUSTERING ####################################\n",
    "################################################################################\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(10000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "biblewords = bag_of_words(bible_doc)\n",
    "poemswords = bag_of_words(poems_doc)\n",
    "storieswords = bag_of_words(stories_doc)\n",
    "busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "ballwords = bag_of_words(ball_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "moby_dickwords = bag_of_words(moby_dick_doc)\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "caesarwords = bag_of_words(caesar_doc)\n",
    "hamletwords = bag_of_words(hamlet_doc)\n",
    "macbethwords = bag_of_words(macbeth_doc)\n",
    "leaveswords = bag_of_words(leaves_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + sensewords + biblewords + poemswords + storieswords\n",
    "                   + busterbrownwords + alicewords\n",
    "                   + ballwords + brownwords + thursdaywords + parentswords + moby_dickwords\n",
    "                   + paradisewords + caesarwords + hamletwords + macbethwords + leaveswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bb9caf29e453>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-1e44e0b58bf1>\u001b[0m in \u001b[0;36mbow_features\u001b[1;34m(sentences, common_words)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# Scaffold the data frame and initialize counts to zero.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_sentence'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_source'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_words\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2329\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2331\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2394\u001b[0m         \"\"\"\n\u001b[0;32m   2395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2396\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2397\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_ensure_valid_index\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   2381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m             self._data = self._data.reindex_axis(value.index.copy(), axis=1,\n\u001b[1;32m-> 2383\u001b[1;33m                                                  fill_value=np.nan)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_axis\u001b[1;34m(self, new_index, axis, method, limit, fill_value, copy)\u001b[0m\n\u001b[0;32m   3856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3857\u001b[0m         return self.reindex_indexer(new_index, indexer, axis=axis,\n\u001b[1;32m-> 3858\u001b[1;33m                                     fill_value=fill_value, copy=copy)\n\u001b[0m\u001b[0;32m   3859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3860\u001b[0m     def reindex_indexer(self, new_axis, indexer, axis, fill_value=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   3895\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[0;32m   3896\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[1;32m-> 3897\u001b[1;33m                 for blk in self.blocks]\n\u001b[0m\u001b[0;32m   3898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3899\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3895\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n\u001b[0;32m   3896\u001b[0m                 fill_value if fill_value is not None else blk.fill_value,))\n\u001b[1;32m-> 3897\u001b[1;33m                 for blk in self.blocks]\n\u001b[0m\u001b[0;32m   3898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3899\u001b[0m         \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[1;32m-> 1046\u001b[1;33m                                        allow_fill=True, fill_value=fill_value)\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[0;32m   1465\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'F'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1466\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1467\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m     func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=axis,\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_counts = bow_features(sample, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = sample['text_source']\n",
    "X = np.array(sample.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "############## THIS LED TO A MEMORY ERROR ###################################\n",
    "#############################################################################\n",
    "\n",
    "#from __future__ import print_function\n",
    "#\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.cm as cm\n",
    "#import numpy as np\n",
    "#\n",
    "#\n",
    "#range_n_clusters = [2, 3, 4, 5, 6]\n",
    "#\n",
    "#for n_clusters in range_n_clusters:\n",
    "#    # Create a subplot with 1 row and 2 columns\n",
    "#    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#    fig.set_size_inches(18, 7)\n",
    "#\n",
    "#    # The 1st subplot is the silhouette plot\n",
    "#    # The silhouette coefficient can range from -1\n",
    "#    ax1.set_xlim([-0.1, 1])\n",
    "#    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "#    # plots of individual clusters, to demarcate them clearly.\n",
    "#    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "#\n",
    "#    # Initialize the clusterer with n_clusters value and a random generator\n",
    "#    # seed of 10 for reproducibility.\n",
    "#    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "#    cluster_labels = clusterer.fit_predict(X)\n",
    "#\n",
    "#    # The silhouette_score gives the average value for all the samples.\n",
    "#    # This gives a perspective into the density and separation of the formed\n",
    "#    # clusters\n",
    "#    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#    print(\"For n_clusters =\", n_clusters,\n",
    "#          \"The average silhouette_score is :\", silhouette_avg)\n",
    "#\n",
    "#    # Compute the silhouette scores for each sample\n",
    "#    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "#\n",
    "#    y_lower = 10\n",
    "#    for i in range(n_clusters):\n",
    "#        # Aggregate the silhouette scores for samples belonging to\n",
    "#        # cluster i, and sort them\n",
    "#        ith_cluster_silhouette_values = \\\n",
    "#            sample_silhouette_values[cluster_labels == i]\n",
    "#\n",
    "#        ith_cluster_silhouette_values.sort()\n",
    "#\n",
    "#        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#        y_upper = y_lower + size_cluster_i\n",
    "#\n",
    "#        color = cm.spectral(float(i) / n_clusters)\n",
    "#        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                          0, ith_cluster_silhouette_values,\n",
    "#                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "#\n",
    "#        # Label the silhouette plots with their cluster numbers at the middle\n",
    "#        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "#\n",
    "#        # Compute the new y_lower for next plot\n",
    "#        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "#\n",
    "#    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "#    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "#    ax1.set_ylabel(\"Cluster label\")\n",
    "#\n",
    "#    # The vertical line for average silhouette score of all the values\n",
    "#    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "#\n",
    "#    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "#\n",
    "#    # 2nd Plot showing the actual clusters formed\n",
    "#    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "#    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "#                c=colors, edgecolor='k')\n",
    "#\n",
    "#    # Labeling the clusters\n",
    "#    centers = clusterer.cluster_centers_\n",
    "#    # Draw white circles at cluster centers\n",
    "#    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "#                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "#\n",
    "#    for i, c in enumerate(centers):\n",
    "#        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "#                    s=50, edgecolor='k')\n",
    "#\n",
    "#    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "#    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "#    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "#\n",
    "#    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "#                  \"with n_clusters = %d\" % n_clusters),\n",
    "#                 fontsize=14, fontweight='bold')\n",
    "#\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "############### AFFINITY PROPOGATION LED TO A MEMORY ERROR EVEN WITH THE SAMPLE SIZE OF 30% ####\n",
    "################################################################################################\n",
    "\n",
    "### Using AffinityPropogation for clustering\n",
    "\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# from sklearn import metrics\n",
    "# \n",
    "# # Declare the model and fit it in one statement.\n",
    "# # Note that you can provide arguments to the model, but we didn't.\n",
    "# af = AffinityPropagation().fit(X)\n",
    "# print('Done')\n",
    "# \n",
    "# # Pull the number of clusters and cluster assignments for each data point.\n",
    "# cluster_centers_indices = af.cluster_centers_indices_\n",
    "# n_clusters_ = len(cluster_centers_indices)\n",
    "# labels = af.labels_\n",
    "# \n",
    "# print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list=[]\n",
    "for index, row in sentences.iterrows():\n",
    "    sen = str(row[0])\n",
    "    sentences_list.append(sen)\n",
    "\n",
    "print(sentences_list[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences_list)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################ RESERVE 25% OF YOUR CORPUS AS A TEST SET ######################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences[1]\n",
    "X = sentences[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets using the sentences_tfidf data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(sentences_tfidf, sentences[1])\n",
    "#lr.predict(sentences[1])\n",
    "#print(lr.score())\n",
    "\n",
    "print(cross_val_score(lr, sentences_tfidf, sentences[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "### If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "### Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
