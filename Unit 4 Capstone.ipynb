{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are twelve different authors in the gutenberg corpus. These will be used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "parents = gutenberg.raw('edgeworth-parents.txt')\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "paradise = gutenberg.raw('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)\n",
    "busterbrown = text_cleaner(busterbrown)\n",
    "alice = text_cleaner(alice)\n",
    "ball = text_cleaner(ball)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)\n",
    "parents = text_cleaner(parents)\n",
    "moby_dick = text_cleaner(moby_dick)\n",
    "paradise = text_cleaner(paradise)\n",
    "\n",
    "emma = emma.split('CHAPTER')\n",
    "persuasion = persuasion.split('Chapter')\n",
    "sense = sense.split('CHAPTER')\n",
    "busterbrown = busterbrown.split('II')\n",
    "alice = alice.split('CHAPTER')\n",
    "ball = ball.split('II')\n",
    "brown = brown.split('CHAPTER')\n",
    "thursday = thursday.split('CHAPTER')\n",
    "parents = parents.split('CHAPTER')\n",
    "moby_dick = moby_dick.split('CHAPTER')\n",
    "paradise = paradise.split('Book')\n",
    "\n",
    "emma = emma[1]\n",
    "persuasion = persuasion[1]\n",
    "sense = sense[1]\n",
    "busterbrown = busterbrown[1]\n",
    "alice = alice[1]\n",
    "ball = ball[1]\n",
    "brown = ball[1]\n",
    "thursday = thursday[1]\n",
    "parents = parents[1]\n",
    "moby_dick = moby_dick[1]\n",
    "paradise = paradise[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking to make sure the texts look correct:\n",
    "#print(emma)\n",
    "#print(persuasion)\n",
    "#print(sense)\n",
    "#print(bible[:1000])\n",
    "#print(poems)\n",
    "#print(stories)\n",
    "#print(busterbrown)\n",
    "#print(alice)\n",
    "#print(ball)\n",
    "#print(brown)\n",
    "#print(thursday)\n",
    "#print(parents)\n",
    "#print(moby_dick)\n",
    "#print(paradise)\n",
    "#print(caesar)\n",
    "#print(hamlet)\n",
    "#print(macbeth)\n",
    "#print(leaves)\n",
    "# The cleaner was successful for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "alice_doc = nlp(alice)\n",
    "ball_doc = nlp(ball)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)\n",
    "parents_doc = nlp(parents)\n",
    "moby_dick_doc = nlp(moby_dick)\n",
    "paradise_doc = nlp(paradise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( , I, Emma, Woodhouse, ,, handsome, ,, clever...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  ( , I, Emma, Woodhouse, ,, handsome, ,, clever...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4  (Between, _, them, _, it, was, more, the, inti...  Austen"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "ball_sents = [[sent, \"Chesterton\"] for sent in ball_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "moby_dick_sents = [[sent, \"Melville\"] for sent in moby_dick_doc.sents]\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "\n",
    "# Combine the sentences into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + sense_sents +\n",
    "                         busterbrown_sents + alice_sents + ball_sents + \n",
    "                         brown_sents + thursday_sents + parents_sents + \n",
    "                         moby_dick_sents + paradise_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########## NEED TO DO BOW BEFORE CLUSTERING ####################################\n",
    "################################################################################\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "ballwords = bag_of_words(ball_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "moby_dickwords = bag_of_words(moby_dick_doc)\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + sensewords + busterbrownwords\n",
    "                   + alicewords + ballwords + brownwords + thursdaywords + \n",
    "                   parentswords + moby_dickwords + paradisewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>maintain</th>\n",
       "      <th>stair</th>\n",
       "      <th>slope</th>\n",
       "      <th>telescope</th>\n",
       "      <th>sooner</th>\n",
       "      <th>journalistic</th>\n",
       "      <th>footing</th>\n",
       "      <th>1791</th>\n",
       "      <th>piece</th>\n",
       "      <th>attitude</th>\n",
       "      <th>...</th>\n",
       "      <th>wince</th>\n",
       "      <th>thirteen</th>\n",
       "      <th>post</th>\n",
       "      <th>instant</th>\n",
       "      <th>childishness</th>\n",
       "      <th>severely</th>\n",
       "      <th>perfidious</th>\n",
       "      <th>liberal</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( , I, Emma, Woodhouse, ,, handsome, ,, clever...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3639 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  maintain stair slope telescope sooner journalistic footing 1791 piece  \\\n",
       "0        0     0     0         0      0            0       0    0     0   \n",
       "1        0     0     0         0      0            0       0    0     0   \n",
       "2        0     0     0         0      0            0       0    0     0   \n",
       "3        0     0     0         0      0            0       0    0     0   \n",
       "4        0     0     0         0      0            0       0    0     0   \n",
       "\n",
       "  attitude     ...     wince thirteen post instant childishness severely  \\\n",
       "0        0     ...         0        0    0       0            0        0   \n",
       "1        0     ...         0        0    0       0            0        0   \n",
       "2        0     ...         0        0    0       0            0        0   \n",
       "3        0     ...         0        0    0       0            0        0   \n",
       "4        0     ...         0        0    0       0            0        0   \n",
       "\n",
       "  perfidious liberal                                      text_sentence  \\\n",
       "0          0       0  ( , I, Emma, Woodhouse, ,, handsome, ,, clever...   \n",
       "1          0       0  (She, was, the, youngest, of, the, two, daught...   \n",
       "2          0       0  (Her, mother, had, died, too, long, ago, for, ...   \n",
       "3          0       0  (Sixteen, years, had, Miss, Taylor, been, in, ...   \n",
       "4          0       0  (Between, _, them, _, it, was, more, the, inti...   \n",
       "\n",
       "  text_source  \n",
       "0      Austen  \n",
       "1      Austen  \n",
       "2      Austen  \n",
       "3      Austen  \n",
       "4      Austen  \n",
       "\n",
       "[5 rows x 3639 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1CDDXZIaZLhLUoMMd0lq0P8B2dKo9DiSWhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24fa01a6c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "text_source  Austen  Burgess  Carroll  Chesterton  Edgeworth  Melville  Milton\n",
      "row_0                                                                         \n",
      "0                38        3        1          18         58         1       4\n",
      "1                12        0        3           1          8         0       0\n",
      "2                 1        0        0           0          0         0       0\n",
      "3                10        3       10          58         91         0       1\n",
      "4                 0        0        0           0          0         0       1\n",
      "5                 2        2        3          15         14         0       0\n",
      "6                 0        0        0           0          0         0       1\n",
      "7                 0        0        0           0          0         0       1\n",
      "8               221       34       52         355        311         6      61\n",
      "9                 0        0        0           0          0         0       1\n"
     ]
    }
   ],
   "source": [
    "# Using k-means to cluster together authors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Estimated number of clusters: 242\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6v4rYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24fa0287160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0        0    1    2    3    4    5    6    7    8    9   ...   232  233  \\\n",
      "text_source                                                   ...              \n",
      "Austen         1    1    1   13    1    1    1    1    1    1 ...     0    0   \n",
      "Burgess        0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Carroll        0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Chesterton     0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Edgeworth      0    0    0    1    0    0    0    0    0    0 ...     0    0   \n",
      "Melville       0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Milton         0    0    0    0    0    0    0    0    0    0 ...     1    1   \n",
      "\n",
      "col_0        234  235  236  237  238  239  240  241  \n",
      "text_source                                          \n",
      "Austen         0    0    0    0    0    0    0    0  \n",
      "Burgess        0    0    0    0    0    0    0    0  \n",
      "Carroll        0    0    0    0    0    0    0    0  \n",
      "Chesterton     0    0    0    0    0    0    0    0  \n",
      "Edgeworth      0    0    0    0    0    0    0    0  \n",
      "Melville       0    0    0    0    0    0    0    0  \n",
      "Milton         1    1    1    1    1    1    1    1  \n",
      "\n",
      "[7 rows x 242 columns]\n"
     ]
    }
   ],
   "source": [
    "## Using AffinityPropogation for clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    " \n",
    "# Declare the model and fit it in one statement.\n",
    "# Note that you can provide arguments to the model, but we didn't.\n",
    "af = AffinityPropagation().fit(X)\n",
    "print('Done')\n",
    " \n",
    "# Pull the number of clusters and cluster assignments for each data point.\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    " \n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(Y,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ZvCzPsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24fa022c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0         0   1   2   3   4   5   6   7   8   9   10\n",
      "text_source                                             \n",
      "Austen       283   1   0   0   0   0   0   0   0   0   0\n",
      "Burgess       42   0   0   0   0   0   0   0   0   0   0\n",
      "Carroll       69   0   0   0   0   0   0   0   0   0   0\n",
      "Chesterton   440   1   1   1   1   1   1   1   0   0   0\n",
      "Edgeworth    478   0   0   0   0   0   0   0   1   1   2\n",
      "Melville       7   0   0   0   0   0   0   0   0   0   0\n",
      "Milton        70   0   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(Y,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.\", 'Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.', \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of Emma.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences_list=[]\n",
    "for index, row in sentences.iterrows():\n",
    "    sen = str(row[0])\n",
    "    sentences_list.append(sen)\n",
    "\n",
    "print(sentences_list[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1862\n"
     ]
    }
   ],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences_list)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################ RESERVE 25% OF YOUR CORPUS AS A TEST SET ######################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences[1]\n",
    "X = sentences[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "Original sentence: It was Miss Taylor's loss which first brought grief.\n",
      "Tf_idf vector: {'laid': 0.46150794291526914, 'arthur': 0.32160003759001582, 'open': 0.3934459360056064, 'sir': 0.26990292600679988, 'sure': 0.34381918717179061, 'character': 0.41487197447351803, 'real': 0.40694735746878985}\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test sets using the sentences_tfidf data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "print(n)\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[10])\n",
    "print('Tf_idf vector:', tfidf_bypara[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 40.7664157383\n",
      "Component 0:\n",
      "0\n",
      "(\", How, 's, this, ,, Susan, ?, \", said, he, .)                                                                                                                                                                                              0.686591\n",
      "(The, socialists, said, he, was, cursing, priests, when, he, should, be, cursing, capitalists, .)                                                                                                                                            0.663381\n",
      "(she, said, aloud, ., ')                                                                                                                                                                                                                     0.632884\n",
      "(She, said, that, Susan, never, did, too, little, ,, or, too, much, .)                                                                                                                                                                       0.619454\n",
      "(\", Now, I, have, him, ,, \", said, the, cunning, tempter, to, himself, ., \")                                                                                                                                                                 0.610823\n",
      "(\", Some, of, them, are, wrong, ,, and, I, 've, written, them, out, again, ,, \", said, Susan, .)                                                                                                                                             0.562203\n",
      "(She, always, shows, us, where, the, nicest, flowers, are, to, be, found, in, the, lanes, and, meadows, ,, \", said, they, ., \")                                                                                                              0.555836\n",
      "(\", There, 's, our, purse, ,, \", said, they, ;, \", do, what, you, please, with, it, .)                                                                                                                                                       0.554870\n",
      "(I, _, must, taste, it, ,, \", said, Bab, ,, taking, the, basin, up, greedily, ., \", Wo, n't, you, take, a, spoon, ?, \", said, Susan, ,, trembling, at, the, large, mouthfuls, which, Barbara, sucked, up, with, a, terrible, noise, ., \")    0.538190\n",
      "(\", My, dear, sir, ,, \", said, the, attorney, ,, taking, him, by, the, button, ,, \", you, have, no, scruple, of, stirring, in, this, business, ?, \", \", A, little, ,, \", said, Sir, Arthur, .)                                               0.535812\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "(\", What, can, that, be, ,, sir, ?, \")                                                                                                                           0.797809\n",
      "(Worse, ,, sir, .)                                                                                                                                               0.760940\n",
      "(\", TO, MYSELF, ,, sir, ,, if, you, please, ,, \", replied, Sir, Arthur, .)                                                                                       0.755245\n",
      "(Sir, Arthur, Somers, ,, the, NEW, MAN, ,, did, not, suit, him, ,, and, he, began, to, be, rather, apprehensive, that, he, should, not, suit, Sir, Arthur, .)    0.646110\n",
      "(Sir, Arthur, was, also, a, man, of, wit, and, eloquence, ,, yet, of, plain, dealing, and, humanity, .)                                                          0.635511\n",
      "(Sir, Walter, has, resented, it, .)                                                                                                                              0.632769\n",
      "(Neither, in, law, nor, equity, ,, \", repeated, Sir, Arthur, ,, with, apparent, incredulity, ., \")                                                               0.630701\n",
      "(Name, the, field, ,, sir, .)                                                                                                                                    0.628263\n",
      "(No, you, have, said, enough, ,, \", replied, Sir, Arthur, .)                                                                                                     0.624550\n",
      "(He, recollected, that, he, had, laid, himself, open, before, he, was, sure, of, Sir, Arthur, 's, REAL, character, .)                                            0.615016\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "(she, said, aloud, ., ')                                                                                                                                                                       0.603758\n",
      "(The, socialists, said, he, was, cursing, priests, when, he, should, be, cursing, capitalists, .)                                                                                              0.602113\n",
      "(\", Now, I, have, him, ,, \", said, the, cunning, tempter, to, himself, ., \")                                                                                                                   0.574207\n",
      "(\", There, 's, our, purse, ,, \", said, they, ;, \", do, what, you, please, with, it, .)                                                                                                         0.512566\n",
      "(She, always, shows, us, where, the, nicest, flowers, are, to, be, found, in, the, lanes, and, meadows, ,, \", said, they, ., \")                                                                0.504806\n",
      "(\", In, what, I, said, ?, \", \", You, said, I, was, not, serious, about, being, an, anarchist, ., \")                                                                                            0.471162\n",
      "(\", How, 's, this, ,, Susan, ?, \", said, he, .)                                                                                                                                                0.408032\n",
      "(\", You, have, really, said, enough, to, excite, my, curiosity, ,, \", said, her, mistress, ;, \", pray, send, for, her, immediately, ;, we, can, see, her, before, we, go, out, to, walk, .)    0.390532\n",
      "(said, Barbara, .)                                                                                                                                                                             0.389549\n",
      "(\", It, wo, n't, do, ,, \", said, Barbara, ,, turning, her, back, .)                                                                                                                            0.336997\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "(She, COULD, NOT, SAY, AMEN, .)                                                                                                                                                                                                                     0.541487\n",
      "(When, you, say, ', thank, you, ', for, the, salt, ,, do, you, mean, what, you, say, ?)                                                                                                                                                             0.514645\n",
      "(Give, me, Bradshaw, ,, I, say, !, \")                                                                                                                                                                                                               0.502711\n",
      "(Then, he, said, ,, \", May, I, say, a, word, ,, your, worship, ?, \")                                                                                                                                                                                0.497686\n",
      "(When, you, say, ', the, world, is, round, ,, ', do, you, mean, what, you, say, ?)                                                                                                                                                                  0.483791\n",
      "(We, always, say, what, we, like, to, one, another, ., \")                                                                                                                                                                                           0.463300\n",
      "(You, say, you, are, a, poet, of, law, ;, I, say, you, are, a, contradiction, in, terms, .)                                                                                                                                                         0.461700\n",
      "(He, says, he, will, challenge, me, to, a, duel, ;, and, I, can, not, say, anything, stronger, about, his, mental, state, than, to, say, that, I, think, that, it, is, highly, probable, that, he, will, .)                                         0.446294\n",
      "(Do, you, mean, what, you, say, now, ?, \", Syme, smiled, .)                                                                                                                                                                                         0.439497\n",
      "(With, surprise, ,, but, with, a, curious, pleasure, ,, he, found, Rosamond, Gregory, still, in, his, company, ., \", Mr., Syme, ,, \", she, said, ,, \", do, the, people, who, talk, like, you, and, my, brother, often, mean, what, they, say, ?)    0.421602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "(And, what, did, you, say, ?)                                                                                                                                         0.491689\n",
      "(She, COULD, NOT, SAY, AMEN, .)                                                                                                                                       0.444693\n",
      "(Give, me, Bradshaw, ,, I, say, !, \")                                                                                                                                 0.412009\n",
      "(When, you, say, ', thank, you, ', for, the, salt, ,, do, you, mean, what, you, say, ?)                                                                               0.402633\n",
      "(\", Stay, ,, oh, stay, !, \", cried, Susan, ,, catching, the, skirt, of, his, coat, with, an, eager, ,, trembling, hand, ;, \", a, whole, week, ,, did, you, say, ?)    0.386231\n",
      "(When, you, say, ', the, world, is, round, ,, ', do, you, mean, what, you, say, ?)                                                                                    0.369761\n",
      "(You, say, you, are, a, poet, of, law, ;, I, say, you, are, a, contradiction, in, terms, .)                                                                           0.367965\n",
      "(\", Nothing, can, be, done, without, Susan, !)                                                                                                                        0.363038\n",
      "(Susan, retired, disconsolate, .)                                                                                                                                     0.345786\n",
      "(Susan, closed, the, curtains, ,, and, was, silent, .)                                                                                                                0.339648\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1050, 1401]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-bc6ffe549b98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 204\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1050, 1401]"
     ]
    }
   ],
   "source": [
    "########???????????????????\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, sentences[1])\n",
    "lr.predict(sentences[1])\n",
    "print(lr.score())\n",
    "\n",
    "#print(cross_val_score(lr, sentences_tfidf, sentences[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "### If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "### Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "Original sentence: Between _them_ it was more the intimacy of sisters.\n",
      "Tf_idf vector: {'brought': 0.44599155215510439, 'grief': 0.54508406879654236, 'loss': 0.48046313682391506, 'miss': 0.34026293204148012, 'taylor': 0.39667518775737154}\n"
     ]
    }
   ],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_test_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "print(n)\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_test_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_test[4])\n",
    "print('Tf_idf vector:', tfidf_bypara[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 60.347450341\n",
      "Component 0:\n",
      "0\n",
      "(\", Everything, shall, be, ready, ,, dear, mother, ;, only, do, n't, hurry, yourself, ,, \", said, Susan, .)                                                                                                                                                                                                                      0.545972\n",
      "(And, if, YOU, could, help, it, ,, Susan, ?, \", said, he, .)                                                                                                                                                                                                                                                                     0.524196\n",
      "(\", I, shall, not, forget, it, ,, \", said, Susan, ,, steadily, .)                                                                                                                                                                                                                                                                0.503506\n",
      "(\", That, 's, a, pity, ., \", \", It, ca, n't, be, helped, ,, \", said, Susan, ,, with, a, sigh, ., \", It, ca, n't, be, helped, how, do, you, know, that, ?, \", said, Case, ., \", Sir, ,, DEAR, sir, !, \", cried, she, ,, looking, up, at, him, ,, and, a, sudden, ray, of, hope, beamed, in, her, ingenuous, countenance, ., \")    0.462713\n",
      "(Something, 's, amiss, ,, Susan, ,, \", said, her, mother, ,, raising, herself, as, well, as, she, was, able, in, the, bed, ,, to, examine, her, daughter, 's, countenance, ., \")                                                                                                                                                 0.408536\n",
      "(Would, you, think, it, amiss, ,, then, ,, my, dear, mother, ,, \", said, Susan, ,, stooping, to, kiss, her, \", would, you, think, it, amiss, ,, if, my, father, was, to, stay, with, us, a, week, longer, ?, \", \", Susan, !)                                                                                                     0.404798\n",
      "(\", If, you, do, n't, like, it, ,, I, will, change, it, ,, and, now, you, will, be, so, good, as, to, give, me, Susan, 's, guinea, -, hen, .)                                                                                                                                                                                    0.375938\n",
      "(said, she, ,, to, herself, ;, \", but, I, know, he, will, be, a, little, sorry, too, for, my, poor, lamb, ., \")                                                                                                                                                                                                                  0.367254\n",
      "(\", Take, it, ,, then, ,, child, ,, \", said, he, ,, pulling, it, off, \", I, shall, soon, have, no, coat, to, dry, and, take, my, hat, ,, too, ,, \", said, he, ,, throwing, it, upon, the, ground, .)                                                                                                                             0.366126\n",
      "(\", More, fool, you, ,, \", said, he, ., \")                                                                                                                                                                                                                                                                                       0.362083\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "(\", Not, at, all, ,, sir, .)                                                                                                                                                                                     0.695351\n",
      "(Sir, Arthur, Somers, was, an, excellent, lawyer, ,, and, a, perfectly, honest, man, .)                                                                                                                          0.615031\n",
      "(Now, ,, sir, ,, this, ,, you, see, ,, is, a, lease, in, reversion, ,, which, the, late, Sir, Benjamin, Somers, had, not, ,, by, his, settlement, ,, a, right, to, make, .)                                      0.615014\n",
      "(Well, ,, sir, ,, \", said, the, editor, of, _)                                                                                                                                                                   0.614512\n",
      "(There, 's, only, one, thing, we, have, forgotten, all, this, time, ,, \", said, Sir, Arthur, .)                                                                                                                  0.578368\n",
      "(Sir, Arthur, Somers, had, two, sisters, ,, sensible, ,, benevolent, women, .)                                                                                                                                   0.458512\n",
      "(Look, at, it, ., \", Susan, looked, and, blushed, ;, it, was, written, ,, \", Sir, Arthur, Somers, ,, to, John, Price, ,, debtor, ,, six, dozen, LAMBS, ,, so, much, ., \")                                        0.411747\n",
      "(The, attorney, ,, brightening, up, ,, prepared, to, take, leave, ;, but, he, could, not, persuade, himself, to, take, his, departure, without, making, one, push, at, Sir, Arthur, about, the, agency, ., \")    0.389559\n",
      "(Your, servant, ,, sir, ., \", Price, retired, with, melancholy, feelings, ,, but, not, intimidated, .)                                                                                                           0.337170\n",
      "(This, is, a, curious, mistake, ,, you, see, ,, Sir, Arthur, ;, and, in, filling, up, those, printed, leases, there, 's, always, a, good, chance, of, some, flaw, .)                                             0.334153\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "(\", A, house, of, her, own, !)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.686324\n",
      "(\", Can, you, be, anything, else, when, you, plaster, your, own, house, with, that, God, -, defying, filth, ?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.643069\n",
      "(But, where, is, the, advantage, of, a, house, of, her, own, ?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.630558\n",
      "(And, she, flounced, out, of, the, house, ,, repeating, \", TAKE, A, SPOON, ,, PIG, ,, was, what, you, meant, to, say, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.554959\n",
      "(Miss, Barbara, 's, maid, Betty, was, the, first, person, that, was, visible, at, the, attorney, 's, house, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.477689\n",
      "(Gregory, gave, through, the, trap, the, address, of, an, obscure, public, -, house, on, the, Chiswick, bank, of, the, river, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.416666\n",
      "(The, moment, Price, got, the, money, ,, he, hastened, to, Mr., Case, 's, house, ,, walked, straight, forward, into, his, room, ,, and, laying, the, money, down, upon, his, desk, ,, \", There, ,, Mr., Attorney, ,, are, your, nine, guineas, ;, count, them, ;, now, I, have, done, with, you, ., \")                                                                                                                                                                                                                                                                                       0.373059\n",
      "(There, is, nobody, in, Highbury, who, deserves, him, and, he, has, been, here, a, whole, year, ,, and, has, fitted, up, his, house, so, comfortably, ,, that, it, would, be, a, shame, to, have, him, single, any, longer, and, I, thought, when, he, was, joining, their, hands, to, -, day, ,, he, looked, so, very, much, as, if, he, would, like, to, have, the, same, kind, office, done, for, him, !, I, think, very, well, of, Mr., Elton, ,, and, this, is, the, only, way, I, have, of, doing, him, a, service, ., \", \")                                                           0.331331\n",
      "(No, one, could, dispute, her, right, to, come, ;, the, house, was, her, husband, 's, from, the, moment, of, his, father, 's, decease, ;, but, the, indelicacy, of, her, conduct, was, so, much, the, greater, ,, and, to, a, woman, in, Mrs., Dashwood, 's, situation, ,, with, only, common, feelings, ,, must, have, been, highly, unpleasing, ;, but, in, HER, mind, there, was, a, sense, of, honor, so, keen, ,, a, generosity, so, romantic, ,, that, any, offence, of, the, kind, ,, by, whomsoever, given, or, received, ,, was, to, her, a, source, of, immoveable, disgust, .)    0.327908\n",
      "(She, was, the, youngest, of, the, two, daughters, of, a, most, affectionate, ,, indulgent, father, ;, and, had, ,, in, consequence, of, her, sister, 's, marriage, ,, been, mistress, of, his, house, from, a, very, early, period, .)                                                                                                                                                                                                                                                                                                                                                      0.326611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "(Evan, lived, like, a, man, walking, on, a, borderland, ,, the, borderland, between, this, world, and, another, .)                                                    0.504599\n",
      "(\", That, we, shall, ruin, this, poor, man, ., \")                                                                                                                     0.449720\n",
      "(This, man, only, gave, expression, to, his, sincere, belief, ., \")                                                                                                   0.406931\n",
      "(Well, ,, well, ., \", Evan, went, out, of, the, Court, of, Justice, free, ,, but, strangely, shaken, ,, like, a, sick, man, .)                                        0.399982\n",
      "(\", As, how, ,, man, !, Why, ,, you, said, something, about, its, not, belonging, to, me, ,, when, you, heard, me, talk, of, inclosing, it, the, other, day, ., \")    0.376366\n",
      "(For, Heaven, 's, sake, ,, man, ,, \", he, said, ,, \", do, n't, talk, so, much, .)                                                                                     0.362931\n",
      "(The, little, man, who, edited, _)                                                                                                                                    0.346707\n",
      "(He, was, a, young, man, in, a, grey, plaid, ,, and, he, smashed, the, window, .)                                                                                     0.331866\n",
      "(Depend, upon, it, ,, a, man, of, six, or, seven, -, and, -, twenty, can, take, care, of, himself, ., \")                                                              0.317609\n",
      "(For, he, was, a, sincere, man, ,, and, in, spite, of, his, superficial, airs, and, graces, ,, at, root, a, humble, one, .)                                           0.309916\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "(\", A, house, of, her, own, !)                                                                                                                                                                                                                                                                            0.432424\n",
      "(But, where, is, the, advantage, of, a, house, of, her, own, ?)                                                                                                                                                                                                                                           0.402361\n",
      "(And, she, flounced, out, of, the, house, ,, repeating, \", TAKE, A, SPOON, ,, PIG, ,, was, what, you, meant, to, say, .)                                                                                                                                                                                  0.388384\n",
      "(\", Can, you, be, anything, else, when, you, plaster, your, own, house, with, that, God, -, defying, filth, ?)                                                                                                                                                                                            0.375740\n",
      "(\", I, shall, not, forget, it, ,, \", said, Susan, ,, steadily, .)                                                                                                                                                                                                                                         0.334750\n",
      "(Miss, Barbara, 's, maid, Betty, was, the, first, person, that, was, visible, at, the, attorney, 's, house, .)                                                                                                                                                                                            0.314776\n",
      "(SHALL, I, PROCEED, ?, \", \")                                                                                                                                                                                                                                                                              0.296220\n",
      "(\", Take, it, ,, then, ,, child, ,, \", said, he, ,, pulling, it, off, \", I, shall, soon, have, no, coat, to, dry, and, take, my, hat, ,, too, ,, \", said, he, ,, throwing, it, upon, the, ground, .)                                                                                                      0.261162\n",
      "(\", More, fool, you, ,, \", said, he, ., \")                                                                                                                                                                                                                                                                0.259703\n",
      "(The, moment, Price, got, the, money, ,, he, hastened, to, Mr., Case, 's, house, ,, walked, straight, forward, into, his, room, ,, and, laying, the, money, down, upon, his, desk, ,, \", There, ,, Mr., Attorney, ,, are, your, nine, guineas, ;, count, them, ;, now, I, have, done, with, you, ., \")    0.254326\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the test data, then project the test data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis and visualizations and data story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model captured 60.347% variance on the test set versus the 40.766% variance on the training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
