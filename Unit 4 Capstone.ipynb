{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are twelve different authors in the gutenberg corpus. These will be used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "poems = gutenberg.raw('blake-poems.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "parents = gutenberg.raw('edgeworth-parents.txt')\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "paradise = gutenberg.raw('milton-paradise.txt')\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "leaves = gutenberg.raw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "emma = re.sub(r'Chapter \\d+', '', emma)\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "sense = re.sub(r'Chapter \\d+', '', sense)\n",
    "bible = re.sub(r'Chapter \\d+', '', bible)\n",
    "poems = re.sub(r'Chapter \\d+', '', poems)\n",
    "stories = re.sub(r'Chapter \\d+', '', stories)\n",
    "busterbrown = re.sub(r'Chapter \\d+', '', busterbrown)\n",
    "alice = re.sub(r'Chapter \\d+', '', alice)\n",
    "ball = re.sub(r'Chapter \\d+', '', ball)\n",
    "brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "thursday = re.sub(r'Chapter \\d+', '', thursday)\n",
    "parents = re.sub(r'Chapter \\d+', '', parents)\n",
    "moby_dick = re.sub(r'Chapter \\d+', '', moby_dick)\n",
    "paradise = re.sub(r'Chapter \\d+', '', paradise)\n",
    "caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
    "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
    "leaves = re.sub(r'Chapter \\d+', '', leaves)\n",
    "\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)\n",
    "bible = text_cleaner(bible)\n",
    "poems = text_cleaner(poems)\n",
    "stories = text_cleaner(stories)\n",
    "busterbrown = text_cleaner(busterbrown)\n",
    "alice = text_cleaner(alice)\n",
    "ball = text_cleaner(ball)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)\n",
    "parents = text_cleaner(parents)\n",
    "moby_dick = text_cleaner(moby_dick)\n",
    "paradise = text_cleaner(paradise)\n",
    "caesar = text_cleaner(caesar)\n",
    "hamlet = text_cleaner(hamlet)\n",
    "macbeth = text_cleaner(macbeth)\n",
    "leaves = text_cleaner(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking to make sure the texts look correct:\n",
    "#print(emma)\n",
    "#print(persuasion)\n",
    "#print(sense)\n",
    "#print(bible[:1000])\n",
    "#print(poems)\n",
    "#print(stories)\n",
    "#print(busterbrown)\n",
    "#print(alice)\n",
    "#print(ball)\n",
    "#print(brown)\n",
    "#print(thursday)\n",
    "#print(parents)\n",
    "#print(moby_dick)\n",
    "#print(paradise)\n",
    "#print(caesar)\n",
    "#print(hamlet)\n",
    "#print(macbeth)\n",
    "#print(leaves)\n",
    "# The cleaner was successful for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)\n",
    "bible_doc = nlp(bible)\n",
    "poems_doc = nlp(poems)\n",
    "stories_doc = nlp(stories)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "alice_doc = nlp(alice)\n",
    "ball_doc = nlp(ball)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)\n",
    "parents_doc = nlp(parents)\n",
    "moby_dick_doc = nlp(moby_dick)\n",
    "paradise_doc = nlp(paradise)\n",
    "caesar_doc = nlp(caesar)\n",
    "hamlet_doc = nlp(hamlet)\n",
    "macbeth_doc = nlp(macbeth)\n",
    "leaves_doc = nlp(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4  (Between, _, them, _, it, was, more, the, inti...  Austen"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "bible_sents = [[sent, \"KJV\"] for sent in bible_doc.sents]\n",
    "poems_sents = [[sent, \"Blake\"] for sent in poems_doc.sents]\n",
    "stories_sents = [[sent, \"Bryant\"] for sent in stories_doc.sents]\n",
    "busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "ball_sents = [[sent, \"Chesterton\"] for sent in ball_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "moby_dick_sents = [[sent, \"Melville\"] for sent in moby_dick_doc.sents]\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "caesar_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "hamlet_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "macbeth_sents = [[sent, \"Shakespeare\"] for sent in macbeth_doc.sents]\n",
    "leaves_sents = [[sent, \"Whitman\"] for sent in leaves_doc.sents]\n",
    "\n",
    "# Combine the sentences into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + sense_sents + bible_sents\n",
    "                         + poems_sents + stories_sents + busterbrown_sents + \n",
    "                         alice_sents + ball_sents + brown_sents + thursday_sents\n",
    "                         + parents_sents + moby_dick_sents + paradise_sents + \n",
    "                         caesar_sents + hamlet_sents + macbeth_sents + leaves_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########## NEED TO DO BOW BEFORE CLUSTERING ####################################\n",
    "################################################################################\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "biblewords = bag_of_words(bible_doc)\n",
    "poemswords = bag_of_words(poems_doc)\n",
    "storieswords = bag_of_words(stories_doc)\n",
    "busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "ballwords = bag_of_words(ball_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "moby_dickwords = bag_of_words(moby_dick_doc)\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "caesarwords = bag_of_words(caesar_doc)\n",
    "hamletwords = bag_of_words(hamlet_doc)\n",
    "macbethwords = bag_of_words(macbeth_doc)\n",
    "leaveswords = bag_of_words(leaves_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(hamletwords + leaveswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "Processing row 8000\n",
      "Processing row 8500\n",
      "Processing row 9000\n",
      "Processing row 9500\n",
      "Processing row 10000\n",
      "Processing row 10500\n",
      "Processing row 11000\n",
      "Processing row 11500\n",
      "Processing row 12000\n",
      "Processing row 12500\n",
      "Processing row 13000\n",
      "Processing row 13500\n",
      "Processing row 14000\n",
      "Processing row 14500\n",
      "Processing row 15000\n",
      "Processing row 15500\n",
      "Processing row 16000\n",
      "Processing row 16500\n",
      "Processing row 17000\n",
      "Processing row 17500\n",
      "Processing row 18000\n",
      "Processing row 18500\n",
      "Processing row 19000\n",
      "Processing row 19500\n",
      "Processing row 20000\n",
      "Processing row 20500\n",
      "Processing row 21000\n",
      "Processing row 21500\n",
      "Processing row 22000\n",
      "Processing row 22500\n",
      "Processing row 23000\n",
      "Processing row 23500\n",
      "Processing row 24000\n",
      "Processing row 24500\n",
      "Processing row 25000\n",
      "Processing row 25500\n",
      "Processing row 26000\n",
      "Processing row 26500\n",
      "Processing row 27000\n",
      "Processing row 27500\n",
      "Processing row 28000\n",
      "Processing row 28500\n",
      "Processing row 29000\n",
      "Processing row 29500\n",
      "Processing row 30000\n",
      "Processing row 30500\n",
      "Processing row 31000\n",
      "Processing row 31500\n",
      "Processing row 32000\n",
      "Processing row 32500\n",
      "Processing row 33000\n",
      "Processing row 33500\n",
      "Processing row 34000\n",
      "Processing row 34500\n",
      "Processing row 35000\n",
      "Processing row 35500\n",
      "Processing row 36000\n",
      "Processing row 36500\n",
      "Processing row 37000\n",
      "Processing row 37500\n",
      "Processing row 38000\n",
      "Processing row 38500\n",
      "Processing row 39000\n",
      "Processing row 39500\n",
      "Processing row 40000\n",
      "Processing row 40500\n",
      "Processing row 41000\n",
      "Processing row 41500\n",
      "Processing row 42000\n",
      "Processing row 42500\n",
      "Processing row 43000\n",
      "Processing row 43500\n",
      "Processing row 44000\n",
      "Processing row 44500\n",
      "Processing row 45000\n",
      "Processing row 45500\n",
      "Processing row 46000\n",
      "Processing row 46500\n",
      "Processing row 47000\n",
      "Processing row 47500\n",
      "Processing row 48000\n",
      "Processing row 48500\n",
      "Processing row 49000\n",
      "Processing row 49500\n",
      "Processing row 50000\n",
      "Processing row 50500\n",
      "Processing row 51000\n",
      "Processing row 51500\n",
      "Processing row 52000\n",
      "Processing row 52500\n",
      "Processing row 53000\n",
      "Processing row 53500\n",
      "Processing row 54000\n",
      "Processing row 54500\n",
      "Processing row 55000\n",
      "Processing row 55500\n",
      "Processing row 56000\n",
      "Processing row 56500\n",
      "Processing row 57000\n",
      "Processing row 57500\n",
      "Processing row 58000\n",
      "Processing row 58500\n",
      "Processing row 59000\n",
      "Processing row 59500\n",
      "Processing row 60000\n",
      "Processing row 60500\n",
      "Processing row 61000\n",
      "Processing row 61500\n",
      "Processing row 62000\n",
      "Processing row 62500\n",
      "Processing row 63000\n",
      "Processing row 63500\n",
      "Processing row 64000\n",
      "Processing row 64500\n",
      "Processing row 65000\n",
      "Processing row 65500\n",
      "Processing row 66000\n",
      "Processing row 66500\n",
      "Processing row 67000\n",
      "Processing row 67500\n",
      "Processing row 68000\n",
      "Processing row 68500\n",
      "Processing row 69000\n",
      "Processing row 69500\n",
      "Processing row 70000\n",
      "Processing row 70500\n",
      "Processing row 71000\n",
      "Processing row 71500\n",
      "Processing row 72000\n",
      "Processing row 72500\n",
      "Processing row 73000\n",
      "Processing row 73500\n",
      "Processing row 74000\n",
      "Processing row 74500\n",
      "Processing row 75000\n",
      "Processing row 75500\n",
      "Processing row 76000\n",
      "Processing row 76500\n",
      "Processing row 77000\n",
      "Processing row 77500\n",
      "Processing row 78000\n",
      "Processing row 78500\n",
      "Processing row 79000\n",
      "Processing row 79500\n",
      "Processing row 80000\n",
      "Processing row 80500\n",
      "Processing row 81000\n",
      "Processing row 81500\n",
      "Processing row 82000\n",
      "Processing row 82500\n",
      "Processing row 83000\n",
      "Processing row 83500\n",
      "Processing row 84000\n",
      "Processing row 84500\n",
      "Processing row 85000\n",
      "Processing row 85500\n",
      "Processing row 86000\n",
      "Processing row 86500\n",
      "Processing row 87000\n",
      "Processing row 87500\n",
      "Processing row 88000\n",
      "Processing row 88500\n",
      "Processing row 89000\n",
      "Processing row 89500\n",
      "Processing row 90000\n",
      "Processing row 90500\n",
      "Processing row 91000\n",
      "Processing row 91500\n",
      "Processing row 92000\n",
      "Processing row 92500\n",
      "Processing row 93000\n",
      "Processing row 93500\n",
      "Processing row 94000\n",
      "Processing row 94500\n",
      "Processing row 95000\n",
      "Processing row 95500\n",
      "Processing row 96000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basket</th>\n",
       "      <th>slow</th>\n",
       "      <th>equal</th>\n",
       "      <th>keep</th>\n",
       "      <th>emulate</th>\n",
       "      <th>ask'd</th>\n",
       "      <th>height</th>\n",
       "      <th>womanhood</th>\n",
       "      <th>behou</th>\n",
       "      <th>think</th>\n",
       "      <th>...</th>\n",
       "      <th>heauy</th>\n",
       "      <th>low</th>\n",
       "      <th>doore</th>\n",
       "      <th>bend</th>\n",
       "      <th>blanket</th>\n",
       "      <th>joy</th>\n",
       "      <th>sunne</th>\n",
       "      <th>amid</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  basket slow equal keep emulate ask'd height womanhood behou think  \\\n",
       "0      0    0     0    0       0     0      0         0     0     0   \n",
       "1      0    0     0    0       0     0      0         0     0     0   \n",
       "2      0    0     0    0       0     0      0         0     0     0   \n",
       "3      0    0     0    0       0     0      0         0     0     0   \n",
       "4      0    0     0    0       0     0      0         0     0     0   \n",
       "\n",
       "      ...     heauy low doore bend blanket joy sunne amid  \\\n",
       "0     ...         0   0     0    0       0   0     0    0   \n",
       "1     ...         0   0     0    0       0   0     0    0   \n",
       "2     ...         0   0     0    0       0   0     0    0   \n",
       "3     ...         0   0     0    0       0   0     0    0   \n",
       "4     ...         0   0     0    0       0   0     0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...      Austen  \n",
       "1  (She, was, the, youngest, of, the, two, daught...      Austen  \n",
       "2  (Her, mother, had, died, too, long, ago, for, ...      Austen  \n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...      Austen  \n",
       "4  (Between, _, them, _, it, was, more, the, inti...      Austen  \n",
       "\n",
       "[5 rows x 3432 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basket</th>\n",
       "      <th>slow</th>\n",
       "      <th>equal</th>\n",
       "      <th>keep</th>\n",
       "      <th>emulate</th>\n",
       "      <th>ask'd</th>\n",
       "      <th>height</th>\n",
       "      <th>womanhood</th>\n",
       "      <th>behou</th>\n",
       "      <th>think</th>\n",
       "      <th>...</th>\n",
       "      <th>heauy</th>\n",
       "      <th>low</th>\n",
       "      <th>doore</th>\n",
       "      <th>bend</th>\n",
       "      <th>blanket</th>\n",
       "      <th>joy</th>\n",
       "      <th>sunne</th>\n",
       "      <th>amid</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, ,, alas, !, the, practices, of, whalemen...</td>\n",
       "      <td>Melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95071</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Why, wag, your, head, with, turban, bound, ,,...</td>\n",
       "      <td>Whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35652</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(119:142, Thy, righteousness, is, an, everlast...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8607</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(\", It, would, not, be, a, great, match, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(And, God, said, unto, Jacob, ,, Arise, ,, go,...</td>\n",
       "      <td>KJV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      basket slow equal keep emulate ask'd height womanhood behou think  \\\n",
       "78487      0    0     0    0       0     0      0         0     0     0   \n",
       "95071      0    0     0    0       0     0      0         0     0     0   \n",
       "35652      0    0     0    0       0     0      0         0     0     0   \n",
       "8607       0    0     0    0       0     0      0         0     0     0   \n",
       "17175      0    0     0    0       0     0      0         0     0     0   \n",
       "\n",
       "          ...     heauy low doore bend blanket joy sunne amid  \\\n",
       "78487     ...         0   0     0    0       0   0     0    0   \n",
       "95071     ...         0   0     0    0       0   0     0    0   \n",
       "35652     ...         0   0     0    0       0   0     0    0   \n",
       "8607      ...         0   0     0    0       0   0     0    0   \n",
       "17175     ...         0   0     0    0       0   0     0    0   \n",
       "\n",
       "                                           text_sentence text_source  \n",
       "78487  (But, ,, alas, !, the, practices, of, whalemen...    Melville  \n",
       "95071  (Why, wag, your, head, with, turban, bound, ,,...     Whitman  \n",
       "35652  (119:142, Thy, righteousness, is, an, everlast...         KJV  \n",
       "8607   (\", It, would, not, be, a, great, match, for, ...      Austen  \n",
       "17175  (And, God, said, unto, Jacob, ,, Arise, ,, go,...         KJV  \n",
       "\n",
       "[5 rows x 3432 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using the full dataframe causes a memory error. \n",
    "### The below code will take a 30% sample of the dataframe.\n",
    "sample = word_counts.sample(frac = .30, random_state = 25)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = sample['text_source']\n",
    "X = np.array(sample.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "############## THIS LED TO A MEMORY ERROR ###################################\n",
    "#############################################################################\n",
    "\n",
    "#from __future__ import print_function\n",
    "#\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.cm as cm\n",
    "#import numpy as np\n",
    "#\n",
    "#\n",
    "#range_n_clusters = [2, 3, 4, 5, 6]\n",
    "#\n",
    "#for n_clusters in range_n_clusters:\n",
    "#    # Create a subplot with 1 row and 2 columns\n",
    "#    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#    fig.set_size_inches(18, 7)\n",
    "#\n",
    "#    # The 1st subplot is the silhouette plot\n",
    "#    # The silhouette coefficient can range from -1\n",
    "#    ax1.set_xlim([-0.1, 1])\n",
    "#    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "#    # plots of individual clusters, to demarcate them clearly.\n",
    "#    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "#\n",
    "#    # Initialize the clusterer with n_clusters value and a random generator\n",
    "#    # seed of 10 for reproducibility.\n",
    "#    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "#    cluster_labels = clusterer.fit_predict(X)\n",
    "#\n",
    "#    # The silhouette_score gives the average value for all the samples.\n",
    "#    # This gives a perspective into the density and separation of the formed\n",
    "#    # clusters\n",
    "#    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#    print(\"For n_clusters =\", n_clusters,\n",
    "#          \"The average silhouette_score is :\", silhouette_avg)\n",
    "#\n",
    "#    # Compute the silhouette scores for each sample\n",
    "#    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "#\n",
    "#    y_lower = 10\n",
    "#    for i in range(n_clusters):\n",
    "#        # Aggregate the silhouette scores for samples belonging to\n",
    "#        # cluster i, and sort them\n",
    "#        ith_cluster_silhouette_values = \\\n",
    "#            sample_silhouette_values[cluster_labels == i]\n",
    "#\n",
    "#        ith_cluster_silhouette_values.sort()\n",
    "#\n",
    "#        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#        y_upper = y_lower + size_cluster_i\n",
    "#\n",
    "#        color = cm.spectral(float(i) / n_clusters)\n",
    "#        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "#                          0, ith_cluster_silhouette_values,\n",
    "#                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "#\n",
    "#        # Label the silhouette plots with their cluster numbers at the middle\n",
    "#        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "#\n",
    "#        # Compute the new y_lower for next plot\n",
    "#        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "#\n",
    "#    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "#    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "#    ax1.set_ylabel(\"Cluster label\")\n",
    "#\n",
    "#    # The vertical line for average silhouette score of all the values\n",
    "#    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "#\n",
    "#    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "#\n",
    "#    # 2nd Plot showing the actual clusters formed\n",
    "#    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "#    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "#                c=colors, edgecolor='k')\n",
    "#\n",
    "#    # Labeling the clusters\n",
    "#    centers = clusterer.cluster_centers_\n",
    "#    # Draw white circles at cluster centers\n",
    "#    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "#                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "#\n",
    "#    for i, c in enumerate(centers):\n",
    "#        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "#                    s=50, edgecolor='k')\n",
    "#\n",
    "#    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "#    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "#    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "#\n",
    "#    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "#                  \"with n_clusters = %d\" % n_clusters),\n",
    "#                 fontsize=14, fontweight='bold')\n",
    "#\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "############### AFFINITY PROPOGATION LED TO A MEMORY ERROR EVEN WITH THE SAMPLE SIZE OF 30% ####\n",
    "################################################################################################\n",
    "\n",
    "### Using AffinityPropogation for clustering\n",
    "\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# from sklearn import metrics\n",
    "# \n",
    "# # Declare the model and fit it in one statement.\n",
    "# # Note that you can provide arguments to the model, but we didn't.\n",
    "# af = AffinityPropagation().fit(X)\n",
    "# print('Done')\n",
    "# \n",
    "# # Pull the number of clusters and cluster assignments for each data point.\n",
    "# cluster_centers_indices = af.cluster_centers_indices_\n",
    "# n_clusters_ = len(cluster_centers_indices)\n",
    "# labels = af.labels_\n",
    "# \n",
    "# print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VOLUME I CHAPTER I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.\", 'Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.', \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of Emma.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences_list=[]\n",
    "for index, row in sentences.iterrows():\n",
    "    sen = str(row[0])\n",
    "    sentences_list.append(sen)\n",
    "\n",
    "print(sentences_list[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 26085\n"
     ]
    }
   ],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences_list)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################ RESERVE 25% OF YOUR CORPUS AS A TEST SET ######################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences[1]\n",
    "X = sentences[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Even before Miss Taylor had ceased to hold the nominal office of governess, the mildness of her temper had hardly allowed her to impose any restraint; and the shadow of authority being now long passed away, they had been living together as friend and friend very mutually attached, and Emma doing just what she liked; highly esteeming Miss Taylor's judgment, but directed chiefly by her own.\n",
      "Tf_idf vector: {'big': 0.30582602262361896, 'mistake': 0.6814957881989635, 'takes': 0.34544000840696121, 'trouble': 0.30204070668393385, 'church': 0.30868844196709, 'make': 0.20846939456099231, 'little': 0.19562562240441375, 'world': 0.23336477848456311}\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test sets using the sentences_tfidf data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 21.8827828058\n",
      "Component 0:\n",
      "0\n",
      "(3:18, And, they, shall, hearken, to, thy, voice, :, and, thou, shalt, come, ,, thou, and, the, elders, of, Israel, ,, unto, the, king, of, Egypt, ,, and, ye, shall, say, unto, him, ,, The, LORD, God, of, the, Hebrews, hath, met, with, us, :, and, now, let, us, go, ,, we, beseech, thee, ,, three, days, ', journey, into, the, wilderness, ,, that, we, may, sacrifice, to, the, LORD, our, God, .)    0.743914\n",
      "(Then, Manoah, intreated, the, LORD, ,, and, said, ,, O, my, Lord, ,, let, the, man, of, God, which, thou, didst, send, come, again, unto, us, ,, and, teach, us, what, we, shall, do, unto, the, child, that, shall, be, born, .)                                                                                                                                                                             0.732923\n",
      "(And, she, said, unto, him, ,, My, lord, ,, thou, swarest, by, the, LORD, thy, God, unto, thine, handmaid, ,, saying, ,, Assuredly, Solomon, thy, son, shall, reign, after, me, ,, and, he, shall, sit, upon, my, throne, .)                                                                                                                                                                                   0.728515\n",
      "(And, it, shall, be, when, the, LORD, shall, bring, thee, into, the, land, of, the, Canaanites, ,, as, he, sware, unto, thee, and, to, thy, fathers, ,, and, shall, give, it, thee, ,, 13:12, That, thou, shalt, set, apart, unto, the, LORD, all, that, openeth, the, matrix, ,, and, every, firstling, that, cometh, of, a, beast, which, thou, hast, ;, the, males, shall, be, the, LORD, 's, .)            0.714514\n",
      "(Then, she, that, is, mine, enemy, shall, see, it, ,, and, shame, shall, cover, her, which, said, unto, me, ,, Where, is, the, LORD, thy, God, ?)                                                                                                                                                                                                                                                              0.710949\n",
      "(And, Moses, said, before, the, LORD, ,, Behold, ,, I, am, of, uncircumcised, lips, ,, and, how, shall, Pharaoh, hearken, unto, me, ?, 7:1, And, the, LORD, said, unto, Moses, ,, See, ,, I, have, made, thee, a, god, to, Pharaoh, :, and, Aaron, thy, brother, shall, be, thy, prophet, .)                                                                                                                   0.707668\n",
      "(And, the, angel, of, the, LORD, said, unto, her, ,, I, will, multiply, thy, seed, exceedingly, ,, that, it, shall, not, be, numbered, for, multitude, .)                                                                                                                                                                                                                                                      0.687129\n",
      "(And, the, Lord, said, unto, him, ,, Arise, ,, and, go, into, the, city, ,, and, it, shall, be, told, thee, what, thou, must, do, .)                                                                                                                                                                                                                                                                           0.683145\n",
      "(But, thou, ,, O, LORD, ,, shall, endure, for, ever, ;, and, thy, remembrance, unto, all, generations, .)                                                                                                                                                                                                                                                                                                      0.680676\n",
      "(the, LORD, said, unto, him, ,, Therefore, whosoever, slayeth, Cain, ,, vengeance, shall, be, taken, on, him, sevenfold, .)                                                                                                                                                                                                                                                                                    0.677304\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "(11:14)    0.564903\n",
      "(14:11)    0.564903\n",
      "(11:14)    0.564903\n",
      "(11:14)    0.564903\n",
      "(11:14)    0.564903\n",
      "(14:11)    0.564903\n",
      "(11:14)    0.564903\n",
      "(14:11)    0.564903\n",
      "(14:11)    0.564903\n",
      "(11:14)    0.564903\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "(What, shall, I, do, ?, what, shall, I, do, ?, \")                                         0.803663\n",
      "(shall, I, ?)                                                                             0.803663\n",
      "(But, how, shall, I, do, ?)                                                               0.803663\n",
      "(., You, shall, do, as, you, please, afterwards, ., \")                                    0.803663\n",
      "(But, many, that, are, first, shall, be, last, ;, and, the, last, first, .)               0.803663\n",
      "(It, shall, be, done, Syw, .)                                                             0.803663\n",
      "(Shall, I, ?)                                                                             0.803663\n",
      "(You, shall, do, as, you, please, afterwards, ., \")                                       0.803663\n",
      "(But, many, that, are, first, shall, be, last, ;, and, the, last, shall, be, first, .)    0.803663\n",
      "(What, shall, I, do, ?, \")                                                                0.803663\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "(11, Listen)                                                                           0.832054\n",
      "(4:11, Only, Luke, is, with, me, .)                                                    0.831931\n",
      "(11:8, And, after, him, Gabbai, ,, Sallai, ,, nine, hundred, twenty, and, eight, .)    0.831920\n",
      "(11:7)                                                                                 0.831867\n",
      "(11:8)                                                                                 0.831867\n",
      "(1:11)                                                                                 0.831867\n",
      "(3:11)                                                                                 0.831867\n",
      "(11:1)                                                                                 0.831867\n",
      "(11:3)                                                                                 0.831867\n",
      "(11:1)                                                                                 0.831867\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "(79:13)                0.696622\n",
      "(October, 13, ., \")    0.696413\n",
      "(1:13)                 0.696386\n",
      "(13:13)                0.696386\n",
      "(13:9)                 0.696386\n",
      "(13:9)                 0.696386\n",
      "(4:13)                 0.696386\n",
      "(13:8)                 0.696386\n",
      "(13:5)                 0.696386\n",
      "(8:13)                 0.696386\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(sentences_tfidf, sentences[1])\n",
    "#lr.predict(sentences[1])\n",
    "#print(lr.score())\n",
    "\n",
    "print(cross_val_score(lr, sentences_tfidf, sentences[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "### If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "### Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
