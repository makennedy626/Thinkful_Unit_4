{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this project, a series of chapters from eleven different novels will be used. A combination of clustering, unsupervised feature generation, and classification models will be used to classify the texts by author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The eleven novels listed below will be used for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "ball = gutenberg.raw('chesterton-ball.txt')\n",
    "brown = gutenberg.raw('chesterton-brown.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "parents = gutenberg.raw('edgeworth-parents.txt')\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "paradise = gutenberg.raw('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the texts and taking one chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)\n",
    "busterbrown = text_cleaner(busterbrown)\n",
    "alice = text_cleaner(alice)\n",
    "ball = text_cleaner(ball)\n",
    "brown = text_cleaner(brown)\n",
    "thursday = text_cleaner(thursday)\n",
    "parents = text_cleaner(parents)\n",
    "moby_dick = text_cleaner(moby_dick)\n",
    "paradise = text_cleaner(paradise)\n",
    "\n",
    "emma = emma.split('CHAPTER')\n",
    "persuasion = persuasion.split('Chapter')\n",
    "sense = sense.split('CHAPTER')\n",
    "busterbrown = busterbrown.split('II')\n",
    "alice = alice.split('CHAPTER')\n",
    "ball = ball.split('II')\n",
    "brown = brown.split('CHAPTER')\n",
    "thursday = thursday.split('CHAPTER')\n",
    "parents = parents.split('CHAPTER')\n",
    "moby_dick = moby_dick.split('CHAPTER')\n",
    "paradise = paradise.split('Book')\n",
    "\n",
    "emma = emma[1]\n",
    "persuasion = persuasion[1]\n",
    "sense = sense[1]\n",
    "busterbrown = busterbrown[1]\n",
    "alice = alice[1]\n",
    "ball = ball[1]\n",
    "brown = ball[1]\n",
    "thursday = thursday[1]\n",
    "parents = parents[1]\n",
    "moby_dick = moby_dick[1]\n",
    "paradise = paradise[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking to make sure the texts look correct:\n",
    "#print(emma)\n",
    "#print(persuasion)\n",
    "#print(sense)\n",
    "#print(bible[:1000])\n",
    "#print(poems)\n",
    "#print(stories)\n",
    "#print(busterbrown)\n",
    "#print(alice)\n",
    "#print(ball)\n",
    "#print(brown)\n",
    "#print(thursday)\n",
    "#print(parents)\n",
    "#print(moby_dick)\n",
    "#print(paradise)\n",
    "#print(caesar)\n",
    "#print(hamlet)\n",
    "#print(macbeth)\n",
    "#print(leaves)\n",
    "# The cleaner was successful for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)\n",
    "busterbrown_doc = nlp(busterbrown)\n",
    "alice_doc = nlp(alice)\n",
    "ball_doc = nlp(ball)\n",
    "brown_doc = nlp(brown)\n",
    "thursday_doc = nlp(thursday)\n",
    "parents_doc = nlp(parents)\n",
    "moby_dick_doc = nlp(moby_dick)\n",
    "paradise_doc = nlp(paradise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( , I, Emma, Woodhouse, ,, handsome, ,, clever...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  ( , I, Emma, Woodhouse, ,, handsome, ,, clever...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4  (Between, _, them, _, it, was, more, the, inti...  Austen"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "ball_sents = [[sent, \"Chesterton\"] for sent in ball_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "moby_dick_sents = [[sent, \"Melville\"] for sent in moby_dick_doc.sents]\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "\n",
    "# Combine the sentences into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + sense_sents +\n",
    "                         busterbrown_sents + alice_sents + ball_sents + \n",
    "                         brown_sents + thursday_sents + parents_sents + \n",
    "                         moby_dick_sents + paradise_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########## NEED TO DO BOW BEFORE CLUSTERING ####################################\n",
    "################################################################################\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "ballwords = bag_of_words(ball_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "parentswords = bag_of_words(parents_doc)\n",
    "moby_dickwords = bag_of_words(moby_dick_doc)\n",
    "paradisewords = bag_of_words(paradise_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + sensewords + busterbrownwords\n",
    "                   + alicewords + ballwords + brownwords + thursdaywords + \n",
    "                   parentswords + moby_dickwords + paradisewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>belief</th>\n",
       "      <th>humor</th>\n",
       "      <th>cope</th>\n",
       "      <th>frozen</th>\n",
       "      <th>generosity</th>\n",
       "      <th>rebellious</th>\n",
       "      <th>valdarno</th>\n",
       "      <th>respectability</th>\n",
       "      <th>eh</th>\n",
       "      <th>ash</th>\n",
       "      <th>...</th>\n",
       "      <th>lonely</th>\n",
       "      <th>disengage</th>\n",
       "      <th>say</th>\n",
       "      <th>wander</th>\n",
       "      <th>loud</th>\n",
       "      <th>chief</th>\n",
       "      <th>readily</th>\n",
       "      <th>tarsus</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>( , I, Emma, Woodhouse, ,, handsome, ,, clever...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3639 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  belief humor cope frozen generosity rebellious valdarno respectability eh  \\\n",
       "0      0     0    0      0          0          0        0              0  0   \n",
       "1      0     0    0      0          0          0        0              0  0   \n",
       "2      0     0    0      0          0          0        0              0  0   \n",
       "3      0     0    0      0          0          0        0              0  0   \n",
       "4      0     0    0      0          0          0        0              0  0   \n",
       "\n",
       "  ash     ...     lonely disengage say wander loud chief readily tarsus  \\\n",
       "0   0     ...          0         0   0      0    0     0       0      0   \n",
       "1   0     ...          0         0   0      0    0     0       0      0   \n",
       "2   0     ...          0         0   0      0    0     0       0      0   \n",
       "3   0     ...          0         0   0      0    0     0       0      0   \n",
       "4   0     ...          0         0   0      0    0     0       0      0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  ( , I, Emma, Woodhouse, ,, handsome, ,, clever...      Austen  \n",
       "1  (She, was, the, youngest, of, the, two, daught...      Austen  \n",
       "2  (Her, mother, had, died, too, long, ago, for, ...      Austen  \n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...      Austen  \n",
       "4  (Between, _, them, _, it, was, more, the, inti...      Austen  \n",
       "\n",
       "[5 rows x 3639 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a528ddc240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means clusters against the data:\n",
      "text_source  Austen  Burgess  Carroll  Chesterton  Edgeworth  Melville  Milton\n",
      "row_0                                                                         \n",
      "0                38        3        1          18         58         1       4\n",
      "1                12        0        3           1          8         0       0\n",
      "2                 1        0        0           0          0         0       0\n",
      "3                10        3       10          58         91         0       1\n",
      "4                 0        0        0           0          0         0       1\n",
      "5                 2        2        3          15         14         0       0\n",
      "6                 0        0        0           0          0         0       1\n",
      "7                 0        0        0           0          0         0       1\n",
      "8               221       34       52         355        311         6      61\n",
      "9                 0        0        0           0          0         0       1\n"
     ]
    }
   ],
   "source": [
    "# Using k-means to cluster together authors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Estimated number of clusters: 242\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a528e10e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0        0    1    2    3    4    5    6    7    8    9   ...   232  233  \\\n",
      "text_source                                                   ...              \n",
      "Austen         1    1    1   13    1    1    1    1    1    1 ...     0    0   \n",
      "Burgess        0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Carroll        0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Chesterton     0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Edgeworth      0    0    0    1    0    0    0    0    0    0 ...     0    0   \n",
      "Melville       0    0    0    0    0    0    0    0    0    0 ...     0    0   \n",
      "Milton         0    0    0    0    0    0    0    0    0    0 ...     1    1   \n",
      "\n",
      "col_0        234  235  236  237  238  239  240  241  \n",
      "text_source                                          \n",
      "Austen         0    0    0    0    0    0    0    0  \n",
      "Burgess        0    0    0    0    0    0    0    0  \n",
      "Carroll        0    0    0    0    0    0    0    0  \n",
      "Chesterton     0    0    0    0    0    0    0    0  \n",
      "Edgeworth      0    0    0    0    0    0    0    0  \n",
      "Melville       0    0    0    0    0    0    0    0  \n",
      "Milton         1    1    1    1    1    1    1    1  \n",
      "\n",
      "[7 rows x 242 columns]\n"
     ]
    }
   ],
   "source": [
    "## Using AffinityPropogation for clustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    " \n",
    "# Declare the model and fit it in one statement.\n",
    "# Note that you can provide arguments to the model, but we didn't.\n",
    "af = AffinityPropagation().fit(X)\n",
    "print('Done')\n",
    " \n",
    "# Pull the number of clusters and cluster assignments for each data point.\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    " \n",
    "print('Estimated number of clusters: {}'.format(n_clusters_))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(Y,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6ZvCzPsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a528e16470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the assigned categories to the ones in the data:\n",
      "col_0         0   1   2   3   4   5   6   7   8   9   10\n",
      "text_source                                             \n",
      "Austen       283   1   0   0   0   0   0   0   0   0   0\n",
      "Burgess       42   0   0   0   0   0   0   0   0   0   0\n",
      "Carroll       69   0   0   0   0   0   0   0   0   0   0\n",
      "Chesterton   440   1   1   1   1   1   1   1   0   0   0\n",
      "Edgeworth    478   0   0   0   0   0   0   0   1   1   2\n",
      "Melville       7   0   0   0   0   0   0   0   0   0   0\n",
      "Milton        70   0   0   0   0   0   0   0   0   0   0\n"
     ]
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "print(pd.crosstab(Y,labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The selected clustering technique will be K-Means due to its greater distribution of authors across the clusters in comparison to Mean Shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' I Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.\", 'Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.', \"Sixteen years had Miss Taylor been in Mr. Woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of Emma.\"]\n"
     ]
    }
   ],
   "source": [
    "sentences_list=[]\n",
    "for index, row in sentences.iterrows():\n",
    "    sen = str(row[0])\n",
    "    sentences_list.append(sen)\n",
    "\n",
    "print(sentences_list[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1862\n"
     ]
    }
   ],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences_list)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################ RESERVE 25% OF YOUR CORPUS AS A TEST SET ######################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences[1]\n",
    "X = sentences[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050\n",
      "Original sentence: It was Miss Taylor's loss which first brought grief.\n",
      "Tf_idf vector: {'laid': 0.46150794291526914, 'arthur': 0.32160003759001582, 'open': 0.3934459360056064, 'sir': 0.26990292600679988, 'sure': 0.34381918717179061, 'character': 0.41487197447351803, 'real': 0.40694735746878985}\n"
     ]
    }
   ],
   "source": [
    "#splitting into training and test sets using the sentences_tfidf data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "print(n)\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[10])\n",
    "print('Tf_idf vector:', tfidf_bypara[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 40.7330402192\n",
      "Component 0:\n",
      "0\n",
      "(\", How, 's, this, ,, Susan, ?, \", said, he, .)                                                                                                                                                                                              0.687571\n",
      "(The, socialists, said, he, was, cursing, priests, when, he, should, be, cursing, capitalists, .)                                                                                                                                            0.663616\n",
      "(she, said, aloud, ., ')                                                                                                                                                                                                                     0.633790\n",
      "(\", Now, I, have, him, ,, \", said, the, cunning, tempter, to, himself, ., \")                                                                                                                                                                 0.620649\n",
      "(She, said, that, Susan, never, did, too, little, ,, or, too, much, .)                                                                                                                                                                       0.618466\n",
      "(\", Some, of, them, are, wrong, ,, and, I, 've, written, them, out, again, ,, \", said, Susan, .)                                                                                                                                             0.570761\n",
      "(\", There, 's, our, purse, ,, \", said, they, ;, \", do, what, you, please, with, it, .)                                                                                                                                                       0.560759\n",
      "(She, always, shows, us, where, the, nicest, flowers, are, to, be, found, in, the, lanes, and, meadows, ,, \", said, they, ., \")                                                                                                              0.557175\n",
      "(I, _, must, taste, it, ,, \", said, Bab, ,, taking, the, basin, up, greedily, ., \", Wo, n't, you, take, a, spoon, ?, \", said, Susan, ,, trembling, at, the, large, mouthfuls, which, Barbara, sucked, up, with, a, terrible, noise, ., \")    0.540696\n",
      "(\", My, dear, sir, ,, \", said, the, attorney, ,, taking, him, by, the, button, ,, \", you, have, no, scruple, of, stirring, in, this, business, ?, \", \", A, little, ,, \", said, Sir, Arthur, .)                                               0.537378\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "(\", What, can, that, be, ,, sir, ?, \")                                                                                                                           0.795581\n",
      "(\", TO, MYSELF, ,, sir, ,, if, you, please, ,, \", replied, Sir, Arthur, .)                                                                                       0.756607\n",
      "(Worse, ,, sir, .)                                                                                                                                               0.753958\n",
      "(Sir, Arthur, Somers, ,, the, NEW, MAN, ,, did, not, suit, him, ,, and, he, began, to, be, rather, apprehensive, that, he, should, not, suit, Sir, Arthur, .)    0.648025\n",
      "(Sir, Arthur, was, also, a, man, of, wit, and, eloquence, ,, yet, of, plain, dealing, and, humanity, .)                                                          0.634635\n",
      "(Sir, Walter, has, resented, it, .)                                                                                                                              0.633127\n",
      "(Neither, in, law, nor, equity, ,, \", repeated, Sir, Arthur, ,, with, apparent, incredulity, ., \")                                                               0.630096\n",
      "(No, you, have, said, enough, ,, \", replied, Sir, Arthur, .)                                                                                                     0.625731\n",
      "(Name, the, field, ,, sir, .)                                                                                                                                    0.603400\n",
      "(\", Sir, Arthur, stood, in, silence, .)                                                                                                                          0.599315\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "(she, said, aloud, ., ')                                                                                                                                                                       0.604619\n",
      "(The, socialists, said, he, was, cursing, priests, when, he, should, be, cursing, capitalists, .)                                                                                              0.602325\n",
      "(\", Now, I, have, him, ,, \", said, the, cunning, tempter, to, himself, ., \")                                                                                                                   0.583449\n",
      "(\", There, 's, our, purse, ,, \", said, they, ;, \", do, what, you, please, with, it, .)                                                                                                         0.518006\n",
      "(She, always, shows, us, where, the, nicest, flowers, are, to, be, found, in, the, lanes, and, meadows, ,, \", said, they, ., \")                                                                0.506015\n",
      "(\", In, what, I, said, ?, \", \", You, said, I, was, not, serious, about, being, an, anarchist, ., \")                                                                                            0.472822\n",
      "(\", How, 's, this, ,, Susan, ?, \", said, he, .)                                                                                                                                                0.408619\n",
      "(\", You, have, really, said, enough, to, excite, my, curiosity, ,, \", said, her, mistress, ;, \", pray, send, for, her, immediately, ;, we, can, see, her, before, we, go, out, to, walk, .)    0.391022\n",
      "(said, Barbara, .)                                                                                                                                                                             0.389825\n",
      "(The, artists, said, that, the, soul, was, most, spiritual, ,, not, when, freed, from, religion, ,, but, when, freed, from, morality, .)                                                       0.343016\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "(She, COULD, NOT, SAY, AMEN, .)                                                                                                                                                                                                                     0.541608\n",
      "(When, you, say, ', thank, you, ', for, the, salt, ,, do, you, mean, what, you, say, ?)                                                                                                                                                             0.514679\n",
      "(Give, me, Bradshaw, ,, I, say, !, \")                                                                                                                                                                                                               0.497380\n",
      "(When, you, say, ', the, world, is, round, ,, ', do, you, mean, what, you, say, ?)                                                                                                                                                                  0.485256\n",
      "(Then, he, said, ,, \", May, I, say, a, word, ,, your, worship, ?, \")                                                                                                                                                                                0.479374\n",
      "(You, say, you, are, a, poet, of, law, ;, I, say, you, are, a, contradiction, in, terms, .)                                                                                                                                                         0.465454\n",
      "(We, always, say, what, we, like, to, one, another, ., \")                                                                                                                                                                                           0.464019\n",
      "(Do, you, mean, what, you, say, now, ?, \", Syme, smiled, .)                                                                                                                                                                                         0.443800\n",
      "(He, says, he, will, challenge, me, to, a, duel, ;, and, I, can, not, say, anything, stronger, about, his, mental, state, than, to, say, that, I, think, that, it, is, highly, probable, that, he, will, .)                                         0.443284\n",
      "(With, surprise, ,, but, with, a, curious, pleasure, ,, he, found, Rosamond, Gregory, still, in, his, company, ., \", Mr., Syme, ,, \", she, said, ,, \", do, the, people, who, talk, like, you, and, my, brother, often, mean, what, they, say, ?)    0.419353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "(And, what, did, you, say, ?)                                                                                                                                         0.491774\n",
      "(She, COULD, NOT, SAY, AMEN, .)                                                                                                                                       0.444781\n",
      "(Give, me, Bradshaw, ,, I, say, !, \")                                                                                                                                 0.407658\n",
      "(When, you, say, ', thank, you, ', for, the, salt, ,, do, you, mean, what, you, say, ?)                                                                               0.402717\n",
      "(\", Stay, ,, oh, stay, !, \", cried, Susan, ,, catching, the, skirt, of, his, coat, with, an, eager, ,, trembling, hand, ;, \", a, whole, week, ,, did, you, say, ?)    0.383611\n",
      "(You, say, you, are, a, poet, of, law, ;, I, say, you, are, a, contradiction, in, terms, .)                                                                           0.370972\n",
      "(When, you, say, ', the, world, is, round, ,, ', do, you, mean, what, you, say, ?)                                                                                    0.370919\n",
      "(\", Nothing, can, be, done, without, Susan, !)                                                                                                                        0.363914\n",
      "(Susan, retired, disconsolate, .)                                                                                                                                     0.344402\n",
      "(Susan, closed, the, curtains, ,, and, was, silent, .)                                                                                                                0.339641\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD8CAYAAACxUoU3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwMnazKwGnKzNzGrg/wEIgl6kMphpbQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a52921fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 You dirty lunatic, stand up, will you?\n",
      "1 He had returned to a late dinner, after some days' absence, and now walked up to Hartfield to say that all were well in Brunswick Square.\n",
      "2 \"There's our purse,\" said they; \"do what you please with it.\n",
      "3 Even if the people were not \"artists,\" the whole was nevertheless artistic.\n",
      "4 It was Miss Taylor's loss which first brought grief.\n",
      "5 Sir Arthur wished to enlarge his domain, and to make a ride round it.\n",
      "6 Are not sheep killed every day, and don't you eat mutton?\n",
      "7 Mrs. Price was an intelligent, active, domestic woman; but her health was not robust.\n",
      "8 And as not unfrequently happens to middle-aged gentlemen when they see a new youth opening before them, he found himself in the presence of the police.\n",
      "9 So thick bestrown, Abject and lost, lay these, covering the flood, Under amazement of their hideous change.\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 130) (1050,)\n",
      "Training set score: 0.827619047619\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_train_lsa.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The visual above shows that the tfidf vectorization technique did a satisfactory job of creating unique features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n",
      "Original sentence: Between _them_ it was more the intimacy of sisters.\n",
      "Tf_idf vector: {'satisfy': 0.43907496133756208, 'send': 0.42140135859773781, 'lease': 0.33251062208716348, 'stir': 0.43907496133756208, 'business': 0.32567753140247291, 'sir': 0.24644745871611551, 'good': 0.2501162797206628, 'home': 0.31132552107830808}\n"
     ]
    }
   ],
   "source": [
    "#Reshapes the vectorizer output into something people can read\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "print(n)\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_test_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_test[4])\n",
    "print('Tf_idf vector:', tfidf_bypara[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 60.3348105032\n",
      "Component 0:\n",
      "0\n",
      "(\", Everything, shall, be, ready, ,, dear, mother, ;, only, do, n't, hurry, yourself, ,, \", said, Susan, .)                                                                                                                                                                                                                      0.545845\n",
      "(And, if, YOU, could, help, it, ,, Susan, ?, \", said, he, .)                                                                                                                                                                                                                                                                     0.529042\n",
      "(\", I, shall, not, forget, it, ,, \", said, Susan, ,, steadily, .)                                                                                                                                                                                                                                                                0.502366\n",
      "(\", That, 's, a, pity, ., \", \", It, ca, n't, be, helped, ,, \", said, Susan, ,, with, a, sigh, ., \", It, ca, n't, be, helped, how, do, you, know, that, ?, \", said, Case, ., \", Sir, ,, DEAR, sir, !, \", cried, she, ,, looking, up, at, him, ,, and, a, sudden, ray, of, hope, beamed, in, her, ingenuous, countenance, ., \")    0.467912\n",
      "(Something, 's, amiss, ,, Susan, ,, \", said, her, mother, ,, raising, herself, as, well, as, she, was, able, in, the, bed, ,, to, examine, her, daughter, 's, countenance, ., \")                                                                                                                                                 0.413323\n",
      "(Would, you, think, it, amiss, ,, then, ,, my, dear, mother, ,, \", said, Susan, ,, stooping, to, kiss, her, \", would, you, think, it, amiss, ,, if, my, father, was, to, stay, with, us, a, week, longer, ?, \", \", Susan, !)                                                                                                     0.410214\n",
      "(\", If, you, do, n't, like, it, ,, I, will, change, it, ,, and, now, you, will, be, so, good, as, to, give, me, Susan, 's, guinea, -, hen, .)                                                                                                                                                                                    0.377337\n",
      "(said, she, ,, to, herself, ;, \", but, I, know, he, will, be, a, little, sorry, too, for, my, poor, lamb, ., \")                                                                                                                                                                                                                  0.365878\n",
      "(\", Take, it, ,, then, ,, child, ,, \", said, he, ,, pulling, it, off, \", I, shall, soon, have, no, coat, to, dry, and, take, my, hat, ,, too, ,, \", said, he, ,, throwing, it, upon, the, ground, .)                                                                                                                             0.360386\n",
      "(\", And, I, wish, Susan, would, come, ,, I, 'm, sure, ,, \", cried, a, little, girl, ,, whose, lap, was, full, of, primroses, ., \")                                                                                                                                                                                               0.358789\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "0\n",
      "(\", Not, at, all, ,, sir, .)                                                                                                                                                                                     0.698893\n",
      "(Well, ,, sir, ,, \", said, the, editor, of, _)                                                                                                                                                                   0.615500\n",
      "(Now, ,, sir, ,, this, ,, you, see, ,, is, a, lease, in, reversion, ,, which, the, late, Sir, Benjamin, Somers, had, not, ,, by, his, settlement, ,, a, right, to, make, .)                                      0.612307\n",
      "(Sir, Arthur, Somers, was, an, excellent, lawyer, ,, and, a, perfectly, honest, man, .)                                                                                                                          0.600435\n",
      "(There, 's, only, one, thing, we, have, forgotten, all, this, time, ,, \", said, Sir, Arthur, .)                                                                                                                  0.582575\n",
      "(Sir, Arthur, Somers, had, two, sisters, ,, sensible, ,, benevolent, women, .)                                                                                                                                   0.457052\n",
      "(Look, at, it, ., \", Susan, looked, and, blushed, ;, it, was, written, ,, \", Sir, Arthur, Somers, ,, to, John, Price, ,, debtor, ,, six, dozen, LAMBS, ,, so, much, ., \")                                        0.412870\n",
      "(The, attorney, ,, brightening, up, ,, prepared, to, take, leave, ;, but, he, could, not, persuade, himself, to, take, his, departure, without, making, one, push, at, Sir, Arthur, about, the, agency, ., \")    0.384944\n",
      "(This, is, a, curious, mistake, ,, you, see, ,, Sir, Arthur, ;, and, in, filling, up, those, printed, leases, there, 's, always, a, good, chance, of, some, flaw, .)                                             0.345752\n",
      "(Your, servant, ,, sir, ., \", Price, retired, with, melancholy, feelings, ,, but, not, intimidated, .)                                                                                                           0.314844\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "0\n",
      "(\", A, house, of, her, own, !)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.685471\n",
      "(\", Can, you, be, anything, else, when, you, plaster, your, own, house, with, that, God, -, defying, filth, ?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.641363\n",
      "(But, where, is, the, advantage, of, a, house, of, her, own, ?)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.627896\n",
      "(And, she, flounced, out, of, the, house, ,, repeating, \", TAKE, A, SPOON, ,, PIG, ,, was, what, you, meant, to, say, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.560854\n",
      "(Miss, Barbara, 's, maid, Betty, was, the, first, person, that, was, visible, at, the, attorney, 's, house, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.477242\n",
      "(Gregory, gave, through, the, trap, the, address, of, an, obscure, public, -, house, on, the, Chiswick, bank, of, the, river, .)                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.403147\n",
      "(The, moment, Price, got, the, money, ,, he, hastened, to, Mr., Case, 's, house, ,, walked, straight, forward, into, his, room, ,, and, laying, the, money, down, upon, his, desk, ,, \", There, ,, Mr., Attorney, ,, are, your, nine, guineas, ;, count, them, ;, now, I, have, done, with, you, ., \")                                                                                                                                                                                                                                                                                       0.370719\n",
      "(No, one, could, dispute, her, right, to, come, ;, the, house, was, her, husband, 's, from, the, moment, of, his, father, 's, decease, ;, but, the, indelicacy, of, her, conduct, was, so, much, the, greater, ,, and, to, a, woman, in, Mrs., Dashwood, 's, situation, ,, with, only, common, feelings, ,, must, have, been, highly, unpleasing, ;, but, in, HER, mind, there, was, a, sense, of, honor, so, keen, ,, a, generosity, so, romantic, ,, that, any, offence, of, the, kind, ,, by, whomsoever, given, or, received, ,, was, to, her, a, source, of, immoveable, disgust, .)    0.342964\n",
      "(There, is, nobody, in, Highbury, who, deserves, him, and, he, has, been, here, a, whole, year, ,, and, has, fitted, up, his, house, so, comfortably, ,, that, it, would, be, a, shame, to, have, him, single, any, longer, and, I, thought, when, he, was, joining, their, hands, to, -, day, ,, he, looked, so, very, much, as, if, he, would, like, to, have, the, same, kind, office, done, for, him, !, I, think, very, well, of, Mr., Elton, ,, and, this, is, the, only, way, I, have, of, doing, him, a, service, ., \", \")                                                           0.329485\n",
      "(She, was, the, youngest, of, the, two, daughters, of, a, most, affectionate, ,, indulgent, father, ;, and, had, ,, in, consequence, of, her, sister, 's, marriage, ,, been, mistress, of, his, house, from, a, very, early, period, .)                                                                                                                                                                                                                                                                                                                                                      0.326975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "0\n",
      "(Evan, lived, like, a, man, walking, on, a, borderland, ,, the, borderland, between, this, world, and, another, .)                                                    0.500214\n",
      "(\", That, we, shall, ruin, this, poor, man, ., \")                                                                                                                     0.450507\n",
      "(Well, ,, well, ., \", Evan, went, out, of, the, Court, of, Justice, free, ,, but, strangely, shaken, ,, like, a, sick, man, .)                                        0.398707\n",
      "(This, man, only, gave, expression, to, his, sincere, belief, ., \")                                                                                                   0.383477\n",
      "(\", As, how, ,, man, !, Why, ,, you, said, something, about, its, not, belonging, to, me, ,, when, you, heard, me, talk, of, inclosing, it, the, other, day, ., \")    0.372583\n",
      "(For, Heaven, 's, sake, ,, man, ,, \", he, said, ,, \", do, n't, talk, so, much, .)                                                                                     0.357982\n",
      "(He, was, a, young, man, in, a, grey, plaid, ,, and, he, smashed, the, window, .)                                                                                     0.355916\n",
      "(The, little, man, who, edited, _)                                                                                                                                    0.345110\n",
      "(Depend, upon, it, ,, a, man, of, six, or, seven, -, and, -, twenty, can, take, care, of, himself, ., \")                                                              0.335854\n",
      "(He, was, a, young, man, ,, born, in, the, Bay, of, Arisaig, ,, opposite, Rum, and, the, Isle, of, Skye, .)                                                           0.306553\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "0\n",
      "(\", A, house, of, her, own, !)                                                                                                                                                                                                                                                                            0.431748\n",
      "(But, where, is, the, advantage, of, a, house, of, her, own, ?)                                                                                                                                                                                                                                           0.400350\n",
      "(And, she, flounced, out, of, the, house, ,, repeating, \", TAKE, A, SPOON, ,, PIG, ,, was, what, you, meant, to, say, .)                                                                                                                                                                                  0.392665\n",
      "(\", Can, you, be, anything, else, when, you, plaster, your, own, house, with, that, God, -, defying, filth, ?)                                                                                                                                                                                            0.374534\n",
      "(\", I, shall, not, forget, it, ,, \", said, Susan, ,, steadily, .)                                                                                                                                                                                                                                         0.334099\n",
      "(Miss, Barbara, 's, maid, Betty, was, the, first, person, that, was, visible, at, the, attorney, 's, house, .)                                                                                                                                                                                            0.314664\n",
      "(SHALL, I, PROCEED, ?, \", \")                                                                                                                                                                                                                                                                              0.295860\n",
      "(\", Take, it, ,, then, ,, child, ,, \", said, he, ,, pulling, it, off, \", I, shall, soon, have, no, coat, to, dry, and, take, my, hat, ,, too, ,, \", said, he, ,, throwing, it, upon, the, ground, .)                                                                                                      0.256996\n",
      "(The, moment, Price, got, the, money, ,, he, hastened, to, Mr., Case, 's, house, ,, walked, straight, forward, into, his, room, ,, and, laying, the, money, down, upon, his, desk, ,, \", There, ,, Mr., Attorney, ,, are, your, nine, guineas, ;, count, them, ;, now, I, have, done, with, you, ., \")    0.253283\n",
      "(\", More, fool, you, ,, \", said, he, ., \")                                                                                                                                                                                                                                                                0.250976\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the test data, then project the test data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD8CAYAAACxUoU3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGF5JREFUeJzt3Xu0XnV95/H3h9xJIuF+SYLBGpSI\nLsAYrc5COoANtguWXbYNjhUsmq5V0Vo7M6V1Flac6dK24jhrqDUiXmoFgdppxmYAbUV7EUhQQBKg\nxIBwjNzvBAjnnM/8sR/0yeHc8/z22fucz4u1V/azn/38vvsQ+J7f892/3/7JNhER0Wz7TPUFRETE\n2JKsIyJaIMk6IqIFkqwjIlogyToiogWSrCMiWiDJOiKixyRdIukBSbeO8L4k/S9J2yXdIumEsdpM\nso6I6L0vAmtHef80YGVnWw98ZqwGk6wjInrM9neBR0Y55Qzgy65cByyRdPhobc7u5QX20vMP7ahl\nauVrVq2rIwwApr7Zog89+3htsQ5dsH9tsQ6cvaiWOA/3P1VLHICn+p+pLdaR8w+uLdZ9ux+rLdad\nD96ovW1jIjln7sG/8DtUPeIXbLC9YQLhlgL3dr3u6xz76UgfaGyyjohoqk5inkhyHmq4Xy6j/rJI\nso6IABgcqDNaH7C86/UyYOdoH0jNOiICYKB//Nve2wi8qzMq5A3A47ZHLIFAetYREQDYgz1rS9Kl\nwEnAQZL6gI8Ac6o4/itgE/BWYDuwC3j3WG0mWUdEAAz2LlnbPnOM9w28byJtJllHRAD0sGddQpJ1\nRATUfYNxwpKsIyJg5vasJb2SapbOUqrxgzuBjbZvKxUzImKy3JtRHsUUGbon6Q+By6gGft8AbO7s\nXyrpvBIxIyL2yuDg+LcpUKpnfQ7wKtvPdx+UdCGwFfj4cB+StJ7OFM6//OR/5z3vGvWGakRE78zQ\nMsggcATw4yHHD++8N6zuKZx1PRskIgKYsTcYPwj8o6Q7+fnDSo4EXg6cWyhmRMTkzcSete2rJB0N\nrKG6wSiqufCbbTf711dEzEwNv8FYbDSIq7mb15VqPyKip6boxuF4ZZx1RATQ9C/9SdYRETAza9YR\nEa2TMkhERAukZx0R0QIDz499zhRKso6IgJRBJquuVcdv2XZZLXEAVh/7ztpiHbbggNpi1blq+5MD\nz9YWqy5z95lTW6y7nrm/tlgHzF1cW6yeSBkkIqIF0rOOiGiBJOuIiOZzbjBGRLRAatYRES2QMkhE\nRAukZx0R0QLpWUdEtEB61hERLdDf7MUHiqxuPhpJ7647ZkTEmDw4/m0K1J6sgY+O9Iak9ZK2SNry\n2DMP1HlNETHTDQ6Of5sCRcogkm4Z6S3g0JE+1726+TGHrMnq5hFRnxlasz4U+GXg0SHHBfxboZgR\nEZPX8NEgpcog3wAW2f7xkO1u4NpCMSMiJq+HNWtJayXdIWm7pPOGef9ISd+W9ANJt0h661htFulZ\n2z5nlPfeUSJmRMRe6dFoEEmzgIuAU4E+YLOkjba3dZ3234DLbX9G0ipgE7BitHan4gZjRETz2OPf\nRrcG2G57h+3dwGXAGUOjAS/p7O8H7Byr0YyzjoiACdWsJa0H1ncd2tAZIAGwFLi3670+4PVDmvgT\n4BpJ7wcWAqeMFTPJOiICJpSsu0euDUPDfWTI6zOBL9r+pKRfBP5a0rH2yAXxJOuICOjl0L0+YHnX\n62W8uMxxDrAWwPb3JM0HDgJGnGCSmnVEBMDAwPi30W0GVko6StJcYB2wccg59wAnA0g6BpgPPDha\no43tWde1COurV/0m82pasHTLrV+pJQ7Aq475jdpiLZmzsLZYD+1+opY4+86aV0scgCPnHVhbrEf6\nn64t1sM1/V31TI/GWdvul3QucDUwC7jE9lZJFwBbbG8E/gD4nKTfpyqRnG2Pfueyscm6LnUl6oho\nuB5OirG9iWo4Xvex87v2twFvmkibMz5ZR0QAM3a6eUREq3iw2Y8jSrKOiIDGPxskyToiAsYzymNK\nJVlHREB61hERrZBkHRHRAmM/oGlKJVlHREDje9bFpptLeqWkkyUtGnJ8bamYERGTNujxb1OgSLKW\n9AHg74H3A7dK6n6W65+WiBkRsVd692yQIkqVQd4LvNb2U5JWAFdKWmH70wz/+EBgz2fEHrropSxZ\ncHChy4uI2JMbXgYplaxn2X4KwPbdkk6iStgvZZRk3f2M2Fce8rpmV/sjYnpp+AzGUjXr+yQd98KL\nTuL+Varntb66UMyIiMnr4YK5JZTqWb8L2GP1Sdv9wLskfbZQzIiIyWt4z7rU6uZ9o7z3ryViRkTs\nlf5MN4+IaL48IjUiogVmYhkkIqJtZurQvYiIdknPOiKiBZKsJ+ehZx+vJc5hCw6oJQ7Uu+L41tsu\nry3Wca86s7ZYda46XpdHB3bVFmuA+r7qL5y9oLZYPZHFByIimi9rMEZEtEGSdUREC2Q0SEREC6Rn\nHRHRAknWERHN54GUQSIimi8964iI5mv60L1iC+ZGRLRKDxfMlbRW0h2Stks6b4RzfkPSNklbJX11\nrDaL9awlrQFse7OkVcBa4Hbbm0rFjIiYtB6VrCXNAi4CTgX6gM2SNtre1nXOSuCPgDfZflTSIWO1\nWyRZS/oIcBowW9I3gdcD1wLnSTre9v8Y4XM/WzB34bxDmD93vxKXFxHxIu7v2Q3GNcB22zsAJF0G\nnAFs6zrnvcBFth8FsP3AWI2W6lm/HTgOmAfcByyz/YSkPweuB4ZN1t0L5h70kqObXUCKiOmld4NB\nlgL3dr3uo+qwdjsaQNK/ArOAP7F91WiNlkrW/bYHgF2SfmT7CQDbz0hq9viYiJiRJnKDsbsK0LGh\n09kE0HDND3k9G1gJnAQsA/5Z0rG2HxspZqlkvVvSvrZ3Aa994aCk/ejl76+IiF6ZQGbqrgIMow9Y\n3vV6GbBzmHOus/08cJekO6iS9+aRYpYaDXJiJ1Fj77Gw2RzgrEIxIyImzYMe9zaGzcBKSUdJmgus\nAzYOOef/AL8EIOkgqrLIjtEaLbW6+XMjHH8IeKhEzIiIvdKj7/y2+yWdC1xNVY++xPZWSRcAW2xv\n7Lz3FknbgAHgv9h+eLR2MykmIgJwfw/bqoYobxpy7PyufQMf6mzjkmQdEQG44XfTkqwjIqDxQx+S\nrCMiSM86IqIVkqwn6dAF+9cSxy8aq17OkjkLa4tV54rjN229tLZYb3zN2bXE2TUw7ICmIp4Z2F1b\nrGMWHFZbrLueG3VwQ+N4YLi5LM3R2GQdEVGn9KwjIlrAg+lZR0Q0XnrWEREtYKdnHRHReOlZR0S0\nwGBGg0RENF9uMEZEtEDTk3Vtq5tL+nJdsSIiJsoe/zYVSi2YO/RB2wJ+SdISANunl4gbETFZTe9Z\nlyqDLKNayfdiqrXHBKwGPjnah7rXNTt88Qr2XzDm6uwRET3R9KF7pcogq4EbgQ8Dj9u+FnjG9nds\nf2ekD9neYHu17dVJ1BFRp4EBjXubCqWW9RoEPiXpis6f95eKFRHRC03vWRdNoLb7gF+X9CvAEyVj\nRUTsjZlas96D7X8A/qGOWBERkzFVozzGK6WJiAjSs46IaIWBwdqmnUxKknVEBCmDRES0wuBMHg0S\nEdEWrR+6J+mVwBnAUqrZiDuBjbZvK3xtERG1aXUZRNIfAmcClwE3dA4vAy6VdJntj5e6sANnLyrV\n9B6eHHi2ljgAD+2ub6j5vrPm1RarrhXHAf7tli/WEqfO1eH3UX09ujpXHB+k4dlviLaXQc4BXmX7\n+e6Dki4EtgLFknVERJ2aPhpkrKsbBI4Y5vjhnfciIqYFT2CbCmP1rD8I/KOkO4F7O8eOBF4OnFvy\nwiIi6tTqMojtqyQdDayhusEooA/YbHughuuLiKhFL0eDSFoLfBqYBVw80v09SW8HrgBeZ3vLaG2O\nORqk8wS96yZ+uRER7dGruq6kWcBFwKl0OreSNtreNuS8xcAHgOvH026zK+oRETUxGvc2hjXAdts7\nbO+mGk13xjDnfQz4M2BcQ9KSrCMigH5r3Juk9ZK2dG3ru5pays/v8UHVu17aHUvS8cBy298Y7/Vl\nBmNEBIynx/zzc+0NwIYR3h6uoZ8NIpG0D/Ap4OwJXF49yVrSf6D6anCr7WvqiBkRMRE9HIvcByzv\ner2Maub3CxYDxwLXqpoQdRiwUdLpo91kLFIGkXRD1/57gf/ducCPSDqvRMyIiL3Rw5r1ZmClpKMk\nzQXWARt/Fsd+3PZBtlfYXkE1gGPURA3latZzuvbXA6fa/ijwFuA/jfSh7jrQT5/+SaFLi4h4scEJ\nbKOx3U81D+Vq4DbgcttbJV0g6fTJXl+pMsg+kvan+mUg2w8C2H5aUv9IH+quA5249OR2PVggIlpt\nYAI167HY3gRsGnLs/BHOPWk8bZZK1vsBN1IV2i3pMNv3SVrE8MX3iIgp1fBVvcok604dZjiDwNtK\nxIyI2BuDDe9H1jp0z/Yu4K46Y0ZEjEfT664ZZx0RQfMfI5pkHREBDNa4CMRkJFlHRABNf4xoknVE\nBDN0NEhERNtkNMgkPdz/1FRfQs/VuYhtnXYNPFdbrLoWsr1p66W1xAF49arfrC3WDx+9u7ZYK5cs\nHfukBslokIiIFkgZJCKiBTJ0LyKiBQbSs46IaL70rCMiWiDJOiKiBZwySERE86VnHRHRApluHhHR\nAk0fZ11qwdzXS3pJZ3+BpI9K+r+SPiFpvxIxIyL2Rq/WYCyl1IK5lwC7Ovufplrm6xOdY18oFDMi\nYtKanqyLLZjbWeEXYLXtEzr7/yLpppE+JGk91WroHL54BfsvOKTQ5UVE7KnpzwYp1bO+VdK7O/s3\nS1oNIOlo4PmRPmR7g+3VtlcnUUdEnQY1/m0qlErW7wHeLOlHwCrge5J2AJ/rvBcR0SgDE9imQqnV\nzR8Hzpa0GHhZJ06f7ftLxIuI2FuDDS+EFB26Z/tJ4OaSMSIieiGTYiIiWqDZ/eok64gIID3riIhW\n6Fez+9ZJ1hERpAwSEdEKKYNM0lP9z9QSZ+4+c2qJA3DkvANri/XowK6xT+qRZwZ21xZrH9UzI6HW\nFce3fa22WGe99g9qi/XdJ++sLVYv9HLonqS1VI/amAVcbPvjQ97/ENWck37gQeC3bf94tDZLTYqJ\niGgVT2AbjaRZwEXAaVSTAs+UtGrIaT+gehTHa4ArgT8b6/qSrCMi6OmDnNYA223vsL0buAw4o/sE\n29+2/cLX3+uAZWM1mmQdEQEM4HFvktZL2tK1re9qailwb9frvs6xkZwD/L+xrq+xNeuIiDpN5Aaj\n7Q3AhhHeHu7GyrDVE0nvBFYDbx4rZpJ1RATg3t1g7AOWd71eBuwcepKkU4APA2+2/dxYjaYMEhFB\nT2vWm4GVko6SNBdYB2zsPkHS8cBngdNtPzCe60vPOiKC3g3ds90v6Vzgaqqhe5fY3irpAmCL7Y3A\nnwOLgCtUDUe9x/bpo7WbZB0RQW9nMNreBGwacuz8rv1TJtpmknVEBNDf8AnnpVY3/4Ck5WOfGRHR\nDJ7AP1Oh1A3GjwHXS/pnSb8r6eDxfKh77OJTzz5S6NIiIl6s6aubl0rWO6iGq3wMeC2wTdJVks7q\nLPU1rO4FcxfNP6DQpUVEvNhM7Vnb9qDta2yfAxwB/CWwliqRR0Q0StN71qVuMO4xg8f281TjDDdK\nWlAoZkTEpA242TcYSyXrEZ8vabueZ59GREzAjFzd3Pa/l2g3IqKUqapFj1fGWUdEkJViIiJaYUaW\nQSIi2iZlkIiIFpipo0EiIlolZZBJOnL+uGao77W7nrm/ljgAj/Q/XVusgRpvlxyz4LDaYt313MO1\nxPnho3fXEgfqXXH8Szd+srZY/3n1H9cWqxdygzEiogVSs46IaIGUQSIiWsC5wRgR0XwD6VlHRDRf\nyiARES2QMkhERAukZx0R0QIzcuiepLnAOmCn7W9JegfwRuA2YENnMYKIiMaYqdPNv9Bpe19JZwGL\ngK8DJwNrgLMKxY2ImJSZWgZ5te3XSJoN/AQ4wvaApK8AN4/0IUnrgfUAL9/vFRy2cGmhy4uI2FPT\nk3WpBXP36ZRCFgP7Avt1js8D5oz0oe7VzZOoI6JOtse9TYVSPevPA7cDs4APA1dI2gG8AbisUMyI\niElres+61BqMn5L0tc7+TklfBk4BPmf7hhIxIyL2xowcDQJVku7afwy4slSsiIi9NeBmPyS1VM06\nIqJVelmzlrRW0h2Stks6b5j350n6Wuf96yWtGKvNJOuICKqa9Xi30UiaBVwEnAasAs6UtGrIaecA\nj9p+OfAp4BNjXV+SdUQEVc16vP+MYQ2w3fYO27upBlWcMeScM4AvdfavBE6WpNEaTbKOiAAG7XFv\nktZL2tK1re9qailwb9frvs4xhjvHdj/wOHDgaNeXZ4NERDCx0SC2NwAbRnh7uB7y0MbHc84ekqwj\nIujpaJA+YHnX62XAzhHO6evM9N4PeGS0RhubrO/b/VgtcQ6Yu7iWOAAP736itlgLZy+oLVZdK45D\nfRMXVi6pbwbtd5+8s7ZYda44/hdb/rS2WL0w2LuZiZuBlZKOonrcxjrgHUPO2Uj1jKTvAW8H/slj\nDDNpbLKOiKhTrybF2O6XdC5wNdUs7ktsb5V0AbDF9kaqWd5/LWk7VY963VjtJllHRNDTnjW2NwGb\nhhw7v2v/WeDXJ9JmknVEBDN4unlERJsMeGCqL2FUSdYREWTB3IiIVpiRj0iNiGib9KwjIlqgl6NB\nSiiWrCX9AvA2qlk6/cCdwKW2Hy8VMyJispo+GqTIg5wkfQD4K2A+8DpgAVXS/p6kk0rEjIjYGwMe\nHPc2FUr1rN8LHNdZ0fxCYJPtkyR9Fvh74PjhPtS9uvnBi45kv/kHFbq8iIg9Nb1mXfIRqS/8IphH\ntco5tu9hnKubJ1FHRJ0m8ojUqVCqZ30xsFnSdcCJdFZBkHQwYzxZKiJiKjS9Z11qdfNPS/oWcAxw\noe3bO8cfpEreERGNMmPHWdveCmwt1X5ERC/NyJ51RETbTNUoj/FKso6IYAZPiomIaJOUQSIiWqDp\nMxiTrCMiSM86IqIVml6zxva02oD10ylOYrUr1nT8maZzrDZtJaebT5X10yxOYrUr1nT8maZzrNaY\njsk6ImLaSbKOiGiB6ZisN0yzOInVrljT8WeazrFaQ52CfkRENNh07FlHREw7SdYRES0wbZK1pLWS\n7pC0XdJ5BeNcIukBSbeWitEVa7mkb0u6TdJWSb9XMNZ8STdIurkT66OlYnXizZL0A0nfKBznbkk/\nlHSTpC2FYy2RdKWk2zt/Z79YKM4rOj/PC9sTkj5YKNbvd/57uFXSpZLml4jTifV7nThbS/08rTbV\nA717NIh+FvAj4GXAXOBmYFWhWCcCJwC31vBzHQ6c0NlfDPx7wZ9LwKLO/hzgeuANBX+2DwFfBb5R\n+N/h3cBBpf+uOrG+BLynsz8XWFJDzFnAfcBLC7S9FLgLWNB5fTlwdqGf41jgVmBfqpnV3wJW1vH3\n1pZtuvSs1wDbbe+wvRu4DDijRCDb36Wmpcls/9T29zv7TwK3Uf0PVCKWbT/VeTmnsxW5+yxpGfAr\nVMu/TQuSXkL1i/zzALZ3236shtAnAz+y/eNC7c8GFkiaTZVIdxaKcwxwne1dtvuB7wBvKxSrlaZL\nsl4K3Nv1uo9CSW2qSFpBtSr89QVjzJJ0E/AA8E3bpWL9T+C/AnU87d3ANZJulFRyZtzLgAeBL3TK\nOxdLWlgw3gvWAZeWaNj2T4C/AO4Bfgo8bvuaErGoetUnSjpQ0r7AW4HlhWK10nRJ1hrm2LQZkyhp\nEfC3wAdtP1Eqju0B28cBy4A1ko7tdQxJvwo8YPvGXrc9gjfZPgE4DXifpFJrgM6mKo99xvbxwNNA\nsXsnAJLmAqcDVxRqf3+qb6hHAUcACyW9s0Qs27dRLaz9TeAqqlJmf4lYbTVdknUfe/4WXka5r2u1\nkjSHKlH/je2v1xGz8/X9WmBtgebfBJwu6W6qctV/lPSVAnEAsL2z8+cDwN9RlcxK6AP6ur6NXEmV\nvEs6Dfi+7fsLtX8KcJftB20/D3wdeGOhWNj+vO0TbJ9IVWq8s1SsNpouyXozsFLSUZ3exjpg4xRf\n016TJKoa6G22Lywc62BJSzr7C6j+R72913Fs/5HtZbZXUP09/ZPtIr01SQslLX5hH3gL1dftnrN9\nH3CvpFd0Dp0MbCsRq8uZFCqBdNwDvEHSvp3/Fk+mum9ShKRDOn8eCfwaZX+21pkWz7O23S/pXOBq\nqrvjl7haXb3nJF0KnAQcJKkP+Ijtz5eIRdUL/S3gh51aMsAf295UINbhwJckzaL6JX657aLD6mpw\nKPB3VZ5hNvBV21cVjPd+4G86HYYdwLtLBerUdU8FfqdUDNvXS7oS+D5VSeIHlJ0K/reSDgSeB95n\n+9GCsVon080jIlpgupRBIiKmtSTriIgWSLKOiGiBJOuIiBZIso6IaIEk64iIFkiyjohogf8P0wxw\nW8G0U1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a552165550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 Gradually as this poor woman collected her scattered thoughts, she recalled the circumstances of the preceding evening.\n",
      "1 And that is just what Buster Bear was doing, and it was very plain to see that he was having great fun.\n",
      "2 Does it occur to you that there is any one article in which we can retrench?\" and Elizabeth, to do her justice, had, in the first ardour of female alarm, set seriously to think what could be done, and had finally proposed these two branches of economy, to cut off some unnecessary charities, and to refrain from new furnishing the drawing-room; to which expedients she afterwards added the happy thought of their taking no present down to Anne, as had been the usual yearly custom.\n",
      "3 And if YOU could help it, Susan?\" said he.\n",
      "4 When you go home, you will be so good, sir, as to send me his lease, that I may satisfy myself before we stir in this business.\"\n",
      "5 Year after year went by, and year after year the death of God in a shop in Ludgate became a less and less important occurrence.\n",
      "6 (Applause.)\n",
      "7 It contained exactly twelve sovereigns.\n",
      "8 Yet not for those,\n",
      "9 The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age.\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_test_lsa) * np.asmatrix(X_test_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_test).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above visual shows that the tfidf vectorization performed well in creating unique features for the ten sample sentences, with the exception of sentences three and eight having a higher correlation of .4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 130) (351,)\n",
      "Test set score: 0.441595441595\n"
     ]
    }
   ],
   "source": [
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_test_lsa.shape, y_test.shape)\n",
    "print('Test set score:', lr.score(X_test_lsa, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Results:\n",
    "\n",
    "### The project used a combination of supervised and unsupervised techniques to classify sentences by author. The Latent Semantic Analysis method captured 40.766% of the variance on the training set and 60.347% of the variance on the test set. This improvement ensures that the model is not subject to overfitting to the training set and that more exploratory analysis could yield a further improvement to results.\n",
    "\n",
    "### The low score for the logistic regression model on the test set can be attributed to the low percent of variance captured in the training set. Improving the lsa model to capture more variance may lead to an increase in test score.\n",
    "\n",
    "### Other options include running logistic regression on the tfidf vectors rather than the lsa vectors as well as increasing the amount of features generated by lsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 99.9864052653\n"
     ]
    }
   ],
   "source": [
    "# Modifying the SVD number of components\n",
    "# The number of components must be less than the number of features.\n",
    "# The tfidf vectorizer created approx. 1800 features, so we will use 1000.\n",
    "svd= TruncatedSVD(1000)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050, 1000) (1050,)\n",
      "Training set score: 0.877142857143\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_train_lsa.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Trying to improve the test set.\n",
    "svd= TruncatedSVD(1000)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the test data, then project the test data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(351, 351) (351,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 351 features per sample; expecting 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-e8880ccf92be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#train = lr.fit(X_train_lsa, y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_lsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test set score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_lsa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 305\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 351 features per sample; expecting 1000"
     ]
    }
   ],
   "source": [
    "#train = lr.fit(X_train_lsa, y_train)\n",
    "print(X_test_lsa.shape, y_test.shape)\n",
    "print('Test set score:', lr.score(X_test_lsa, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
