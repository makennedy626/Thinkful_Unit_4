{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are twelve different authors in the gutenberg corpus. These will be used for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "poems = gutenberg.raw('blake-poems.txt')\n",
    "#stories = gutenberg.raw('bryant-stories.txt')\n",
    "#busterbrown = gutenberg.raw('burgess-busterbrown.txt')\n",
    "#alice = gutenberg.raw('carroll-alice.txt')\n",
    "#ball = gutenberg.raw('chesterton-ball.txt')\n",
    "#brown = gutenberg.raw('chesterton-brown.txt')\n",
    "#thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "#parents = gutenberg.raw('edgeworth-parents.txt')\n",
    "#moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
    "#paradise = gutenberg.raw('milton-paradise.txt')\n",
    "#caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "#hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "#macbeth = gutenberg.raw('shakespeare-macbeth.txt')\n",
    "#leaves = gutenberg.raw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "emma = re.sub(r'Chapter \\d+', '', emma)\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "sense = re.sub(r'Chapter \\d+', '', sense)\n",
    "bible = re.sub(r'Chapter \\d+', '', bible)\n",
    "poems = re.sub(r'Chapter \\d+', '', poems)\n",
    "#stories = re.sub(r'Chapter \\d+', '', stories)\n",
    "#busterbrown = re.sub(r'Chapter \\d+', '', busterbrown)\n",
    "#alice = re.sub(r'Chapter \\d+', '', alice)\n",
    "#ball = re.sub(r'Chapter \\d+', '', ball)\n",
    "#brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "#thursday = re.sub(r'Chapter \\d+', '', thursday)\n",
    "#parents = re.sub(r'Chapter \\d+', '', parents)\n",
    "#moby_dick = re.sub(r'Chapter \\d+', '', moby_dick)\n",
    "#paradise = re.sub(r'Chapter \\d+', '', paradise)\n",
    "#caesar = re.sub(r'Chapter \\d+', '', caesar)\n",
    "#hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "#macbeth = re.sub(r'Chapter \\d+', '', macbeth)\n",
    "#leaves = re.sub(r'Chapter \\d+', '', leaves)\n",
    "\n",
    "emma = text_cleaner(emma)\n",
    "persuasion = text_cleaner(persuasion)\n",
    "sense = text_cleaner(sense)\n",
    "bible = text_cleaner(bible)\n",
    "poems = text_cleaner(poems)\n",
    "#stories = text_cleaner(stories)\n",
    "#busterbrown = text_cleaner(busterbrown)\n",
    "#alice = text_cleaner(alice)\n",
    "#ball = text_cleaner(ball)\n",
    "#brown = text_cleaner(brown)\n",
    "#thursday = text_cleaner(thursday)\n",
    "#parents = text_cleaner(parents)\n",
    "#moby_dick = text_cleaner(moby_dick)\n",
    "#paradise = text_cleaner(paradise)\n",
    "#caesar = text_cleaner(caesar)\n",
    "#hamlet = text_cleaner(hamlet)\n",
    "#macbeth = text_cleaner(macbeth)\n",
    "#leaves = text_cleaner(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Checking to make sure the texts look correct:\n",
    "#print(emma)\n",
    "#print(persuasion)\n",
    "#print(sense)\n",
    "#print(bible[:1000])\n",
    "#print(poems)\n",
    "#print(stories)\n",
    "#print(busterbrown)\n",
    "#print(alice)\n",
    "#print(ball)\n",
    "#print(brown)\n",
    "#print(thursday)\n",
    "#print(parents)\n",
    "#print(moby_dick)\n",
    "#print(paradise)\n",
    "#print(caesar)\n",
    "#print(hamlet)\n",
    "#print(macbeth)\n",
    "#print(leaves)\n",
    "# The cleaner was successful for each of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "emma_doc = nlp(emma)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "sense_doc = nlp(sense)\n",
    "bible_doc = nlp(bible)\n",
    "poems_doc = nlp(poems)\n",
    "#stories_doc = nlp(stories)\n",
    "#busterbrown_doc = nlp(busterbrown)\n",
    "#alice_doc = nlp(alice)\n",
    "#ball_doc = nlp(ball)\n",
    "#brown_doc = nlp(brown)\n",
    "#thursday_doc = nlp(thursday)\n",
    "#parents_doc = nlp(parents)\n",
    "#moby_dick_doc = nlp(moby_dick)\n",
    "#paradise_doc = nlp(paradise)\n",
    "#caesar_doc = nlp(caesar)\n",
    "#hamlet_doc = nlp(hamlet)\n",
    "#macbeth_doc = nlp(macbeth)\n",
    "#leaves_doc = nlp(leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...  Austen\n",
       "1  (She, was, the, youngest, of, the, two, daught...  Austen\n",
       "2  (Her, mother, had, died, too, long, ago, for, ...  Austen\n",
       "3  (Sixteen, years, had, Miss, Taylor, been, in, ...  Austen\n",
       "4  (Between, _, them, _, it, was, more, the, inti...  Austen"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]\n",
    "bible_sents = [[sent, \"KJV\"] for sent in bible_doc.sents]\n",
    "poems_sents = [[sent, \"Blake\"] for sent in poems_doc.sents]\n",
    "#stories_sents = [[sent, \"Bryant\"] for sent in stories_doc.sents]\n",
    "#busterbrown_sents = [[sent, \"Burgess\"] for sent in busterbrown_doc.sents]\n",
    "#alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "#ball_sents = [[sent, \"Chesterton\"] for sent in ball_doc.sents]\n",
    "#brown_sents = [[sent, \"Chesterton\"] for sent in brown_doc.sents]\n",
    "#thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "#parents_sents = [[sent, \"Edgeworth\"] for sent in parents_doc.sents]\n",
    "#moby_dick_sents = [[sent, \"Melville\"] for sent in moby_dick_doc.sents]\n",
    "#paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "#caesar_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "#hamlet_sents = [[sent, \"Shakespeare\"] for sent in caesar_doc.sents]\n",
    "#macbeth_sents = [[sent, \"Shakespeare\"] for sent in macbeth_doc.sents]\n",
    "#leaves_sents = [[sent, \"Whitman\"] for sent in leaves_doc.sents]\n",
    "\n",
    "# Combine the sentences into one data frame.\n",
    "sentences = pd.DataFrame(emma_sents + persuasion_sents + sense_sents + bible_sents\n",
    "                         + poems_sents) #+ stories_sents + busterbrown_sents)\n",
    "                         #+ alice_sents + ball_sents + brown_sents + thursday_sents\n",
    "                         #+ parents_sents + moby_dick_sents + paradise_sents + \n",
    "                         #caesar_sents + hamlet_sents + macbeth_sents + leaves_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Using the full dataframe causes a memory error. \n",
    "### The below code will take a 30% sample of the dataframe.\n",
    "#sample = sentences.sample(frac = .30, random_state = 25)\n",
    "#sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "########## NEED TO DO BOW BEFORE CLUSTERING ####################################\n",
    "################################################################################\n",
    "\n",
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "emmawords = bag_of_words(emma_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "biblewords = bag_of_words(bible_doc)\n",
    "poemswords = bag_of_words(poems_doc)\n",
    "#storieswords = bag_of_words(stories_doc)\n",
    "#busterbrownwords = bag_of_words(busterbrown_doc)\n",
    "#alicewords = bag_of_words(alice_doc)\n",
    "#ballwords = bag_of_words(ball_doc)\n",
    "#brownwords = bag_of_words(brown_doc)\n",
    "#thursdaywords = bag_of_words(thursday_doc)\n",
    "#parentswords = bag_of_words(parents_doc)\n",
    "#moby_dickwords = bag_of_words(moby_dick_doc)\n",
    "#paradisewords = bag_of_words(paradise_doc)\n",
    "#caesarwords = bag_of_words(caesar_doc)\n",
    "#hamletwords = bag_of_words(hamlet_doc)\n",
    "#macbethwords = bag_of_words(macbeth_doc)\n",
    "#leaveswords = bag_of_words(leaves_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(emmawords + persuasionwords + sensewords + biblewords + poemswords) # + storieswords\n",
    "                   #+ busterbrownwords) # + alicewords\n",
    "                   #+ ballwords + brownwords + thursdaywords + parentswords + moby_dickwords\n",
    "                   #+ paradisewords + caesarwords + hamletwords + macbethwords + leaveswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n",
      "Processing row 8000\n",
      "Processing row 8500\n",
      "Processing row 9000\n",
      "Processing row 9500\n",
      "Processing row 10000\n",
      "Processing row 10500\n",
      "Processing row 11000\n",
      "Processing row 11500\n",
      "Processing row 12000\n",
      "Processing row 12500\n",
      "Processing row 13000\n",
      "Processing row 13500\n",
      "Processing row 14000\n",
      "Processing row 14500\n",
      "Processing row 15000\n",
      "Processing row 15500\n",
      "Processing row 16000\n",
      "Processing row 16500\n",
      "Processing row 17000\n",
      "Processing row 17500\n",
      "Processing row 18000\n",
      "Processing row 18500\n",
      "Processing row 19000\n",
      "Processing row 19500\n",
      "Processing row 20000\n",
      "Processing row 20500\n",
      "Processing row 21000\n",
      "Processing row 21500\n",
      "Processing row 22000\n",
      "Processing row 22500\n",
      "Processing row 23000\n",
      "Processing row 23500\n",
      "Processing row 24000\n",
      "Processing row 24500\n",
      "Processing row 25000\n",
      "Processing row 25500\n",
      "Processing row 26000\n",
      "Processing row 26500\n",
      "Processing row 27000\n",
      "Processing row 27500\n",
      "Processing row 28000\n",
      "Processing row 28500\n",
      "Processing row 29000\n",
      "Processing row 29500\n",
      "Processing row 30000\n",
      "Processing row 30500\n",
      "Processing row 31000\n",
      "Processing row 31500\n",
      "Processing row 32000\n",
      "Processing row 32500\n",
      "Processing row 33000\n",
      "Processing row 33500\n",
      "Processing row 34000\n",
      "Processing row 34500\n",
      "Processing row 35000\n",
      "Processing row 35500\n",
      "Processing row 36000\n",
      "Processing row 36500\n",
      "Processing row 37000\n",
      "Processing row 37500\n",
      "Processing row 38000\n",
      "Processing row 38500\n",
      "Processing row 39000\n",
      "Processing row 39500\n",
      "Processing row 40000\n",
      "Processing row 40500\n",
      "Processing row 41000\n",
      "Processing row 41500\n",
      "Processing row 42000\n",
      "Processing row 42500\n",
      "Processing row 43000\n",
      "Processing row 43500\n",
      "Processing row 44000\n",
      "Processing row 44500\n",
      "Processing row 45000\n",
      "Processing row 45500\n",
      "Processing row 46000\n",
      "Processing row 46500\n",
      "Processing row 47000\n",
      "Processing row 47500\n",
      "Processing row 48000\n",
      "Processing row 48500\n",
      "Processing row 49000\n",
      "Processing row 49500\n",
      "Processing row 50000\n",
      "Processing row 50500\n",
      "Processing row 51000\n",
      "Processing row 51500\n",
      "Processing row 52000\n",
      "Processing row 52500\n",
      "Processing row 53000\n",
      "Processing row 53500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encamp</th>\n",
       "      <th>syria</th>\n",
       "      <th>ashamed</th>\n",
       "      <th>reformation</th>\n",
       "      <th>person</th>\n",
       "      <th>continually</th>\n",
       "      <th>ignorance</th>\n",
       "      <th>affront</th>\n",
       "      <th>cordiality</th>\n",
       "      <th>smild</th>\n",
       "      <th>...</th>\n",
       "      <th>circumcise</th>\n",
       "      <th>elegance</th>\n",
       "      <th>belly</th>\n",
       "      <th>unknown</th>\n",
       "      <th>effect</th>\n",
       "      <th>henrietta</th>\n",
       "      <th>dwell</th>\n",
       "      <th>9:12</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(She, was, the, youngest, of, the, two, daught...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Her, mother, had, died, too, long, ago, for, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Sixteen, years, had, Miss, Taylor, been, in, ...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Between, _, them, _, it, was, more, the, inti...</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4715 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  encamp syria ashamed reformation person continually ignorance affront  \\\n",
       "0      0     0       0           0      0           0         0       0   \n",
       "1      0     0       0           0      0           0         0       0   \n",
       "2      0     0       0           0      0           0         0       0   \n",
       "3      0     0       0           0      0           0         0       0   \n",
       "4      0     0       0           0      0           0         0       0   \n",
       "\n",
       "  cordiality smild     ...     circumcise elegance belly unknown effect  \\\n",
       "0          0     0     ...              0        0     0       0      0   \n",
       "1          0     0     ...              0        0     0       0      0   \n",
       "2          0     0     ...              0        0     0       0      0   \n",
       "3          0     0     ...              0        0     0       0      0   \n",
       "4          0     0     ...              0        0     0       0      0   \n",
       "\n",
       "  henrietta dwell 9:12                                      text_sentence  \\\n",
       "0         0     0    0  (VOLUME, I, CHAPTER, I, Emma, Woodhouse, ,, ha...   \n",
       "1         0     0    0  (She, was, the, youngest, of, the, two, daught...   \n",
       "2         0     0    0  (Her, mother, had, died, too, long, ago, for, ...   \n",
       "3         0     0    0  (Sixteen, years, had, Miss, Taylor, been, in, ...   \n",
       "4         0     0    0  (Between, _, them, _, it, was, more, the, inti...   \n",
       "\n",
       "  text_source  \n",
       "0      Austen  \n",
       "1      Austen  \n",
       "2      Austen  \n",
       "3      Austen  \n",
       "4      Austen  \n",
       "\n",
       "[5 rows x 4715 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-14c8299735f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Calculate predicted values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Plot the solution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m         \"\"\"\n\u001b[1;32m--> 917\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 896\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    897\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[1;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    399\u001b[0m     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n\u001b[0;32m    400\u001b[0m                                             max_iter=max_iter, verbose=verbose)\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0minertia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using k-means to cluster together authors\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=4, random_state=42).fit_predict(X)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.show()\n",
    "\n",
    "# Check the solution against the data.\n",
    "print('Comparing k-means clusters against the data:')\n",
    "print(pd.crosstab(y_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c8a3754405c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# This gives a perspective into the density and separation of the formed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0msilhouette_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     print(\"For n_clusters =\", n_clusters,\n\u001b[0;32m     39\u001b[0m           \"The average silhouette_score is :\", silhouette_avg)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mcheck_number_of_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[0munique_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0mn_samples_per_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[1;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1090\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1091\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \u001b[1;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[0mYY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCkAAAGfCAYAAAByXYioAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAgCH8PyRb75S1Ms/3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x225b5b1e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################################\n",
    "############## THIS LED TO A MEMORY ERROR ###################################\n",
    "#############################################################################\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "############### AFFINITY PROPOGATION LED TO A MEMORY ERROR EVEN WITH THE SAMPLE SIZE OF 30% ####\n",
    "################################################################################################\n",
    "\n",
    "### Using AffinityPropogation for clustering\n",
    "\n",
    "# from sklearn.cluster import AffinityPropagation\n",
    "# from sklearn import metrics\n",
    "# \n",
    "# # Declare the model and fit it in one statement.\n",
    "# # Note that you can provide arguments to the model, but we didn't.\n",
    "# af = AffinityPropagation().fit(X)\n",
    "# print('Done')\n",
    "# \n",
    "# # Pull the number of clusters and cluster assignments for each data point.\n",
    "# cluster_centers_indices = af.cluster_centers_indices_\n",
    "# n_clusters_ = len(cluster_centers_indices)\n",
    "# labels = af.labels_\n",
    "# \n",
    "# print('Estimated number of clusters: {}'.format(n_clusters_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_list=[]\n",
    "for index, row in sentences.iterrows():\n",
    "    sen = str(row[0])\n",
    "    sentences_list.append(sen)\n",
    "\n",
    "print(sentences_list[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using tf-idf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences_list)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################ RESERVE 25% OF YOUR CORPUS AS A TEST SET ######################\n",
    "################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences[1]\n",
    "X = sentences[0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#splitting into training and test sets using the sentences_tfidf data\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(sentences_tfidf, sentences[1])\n",
    "#lr.predict(sentences[1])\n",
    "#print(lr.score())\n",
    "\n",
    "print(cross_val_score(lr, sentences_tfidf, sentences[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "### If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "### Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
